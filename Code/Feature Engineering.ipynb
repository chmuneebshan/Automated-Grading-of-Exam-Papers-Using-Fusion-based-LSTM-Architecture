{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2c95ee24",
      "metadata": {
        "id": "2c95ee24"
      },
      "source": [
        "# 0) Setting Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c714d3d0",
      "metadata": {
        "id": "c714d3d0"
      },
      "outputs": [],
      "source": [
        "# # Installing Required Modules\n",
        "# !pip install nltk\n",
        "# !pip install textstat\n",
        "# !pip install gensim\n",
        "# !pip install scipy==1.11.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f36dc5a1",
      "metadata": {
        "id": "f36dc5a1"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import string\n",
        "import gensim\n",
        "import textstat\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk import pos_tag\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.corpora import Dictionary\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from gensim.models import LdaModel, LsiModel, Doc2Vec\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91cce177",
      "metadata": {
        "id": "91cce177",
        "outputId": "646fd4c6-317e-43d1-8f26-f2518d6227aa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\ATAUMAR\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\ATAUMAR\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\ATAUMAR\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06134656",
      "metadata": {
        "id": "06134656"
      },
      "source": [
        "# 1) Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c630d519",
      "metadata": {
        "id": "c630d519"
      },
      "outputs": [],
      "source": [
        "# Reading Files\n",
        "train_data = pd.read_csv(\"../Data/Raw/training_set_rel3.tsv\", sep=\"\\t\", encoding='ISO-8859-1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7844fb17",
      "metadata": {
        "id": "7844fb17"
      },
      "outputs": [],
      "source": [
        "# Dropping Irrelavant Columns\n",
        "columns_to_keep = ['essay_id', 'essay_set', 'essay', 'rater1_domain1', 'rater2_domain1',\n",
        "                   'domain1_score']\n",
        "\n",
        "train_data = train_data[columns_to_keep]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f553a6f6",
      "metadata": {
        "id": "f553a6f6"
      },
      "source": [
        "# 2) Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbdac1a2",
      "metadata": {
        "id": "cbdac1a2"
      },
      "source": [
        "## 2.1) Vocabulary-level features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ec69fba",
      "metadata": {
        "id": "1ec69fba"
      },
      "source": [
        "### 2.1.1) Length Based Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c8b3a3b",
      "metadata": {
        "id": "1c8b3a3b",
        "outputId": "24fda81e-7904-4356-ddf0-41c854c1f8ca"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_count</th>\n",
              "      <th>unique_token_count</th>\n",
              "      <th>nostop_count</th>\n",
              "      <th>avg_sentence_length</th>\n",
              "      <th>avg_word_length</th>\n",
              "      <th>sentence_count</th>\n",
              "      <th>long_word_count</th>\n",
              "      <th>short_word_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>338.0</td>\n",
              "      <td>184.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>30.727273</td>\n",
              "      <td>4.550296</td>\n",
              "      <td>11.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>138.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>419.0</td>\n",
              "      <td>216.0</td>\n",
              "      <td>230.0</td>\n",
              "      <td>22.052632</td>\n",
              "      <td>4.463007</td>\n",
              "      <td>19.0</td>\n",
              "      <td>86.0</td>\n",
              "      <td>169.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>279.0</td>\n",
              "      <td>167.0</td>\n",
              "      <td>139.0</td>\n",
              "      <td>18.600000</td>\n",
              "      <td>4.526882</td>\n",
              "      <td>15.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>119.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>524.0</td>\n",
              "      <td>275.0</td>\n",
              "      <td>302.0</td>\n",
              "      <td>20.960000</td>\n",
              "      <td>5.041985</td>\n",
              "      <td>25.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>182.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>465.0</td>\n",
              "      <td>226.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>4.526882</td>\n",
              "      <td>31.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>192.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   word_count  unique_token_count  nostop_count  avg_sentence_length  \\\n",
              "0       338.0               184.0         170.0            30.727273   \n",
              "1       419.0               216.0         230.0            22.052632   \n",
              "2       279.0               167.0         139.0            18.600000   \n",
              "3       524.0               275.0         302.0            20.960000   \n",
              "4       465.0               226.0         229.0            15.000000   \n",
              "\n",
              "   avg_word_length  sentence_count  long_word_count  short_word_count  \n",
              "0         4.550296            11.0             67.0             138.0  \n",
              "1         4.463007            19.0             86.0             169.0  \n",
              "2         4.526882            15.0             56.0             119.0  \n",
              "3         5.041985            25.0            140.0             182.0  \n",
              "4         4.526882            31.0             95.0             192.0  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Set of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def length_based_features(text):\n",
        "    # Tokenize the text into sentences and words using string operations\n",
        "    sentences = text.split('.')\n",
        "    words = text.split()\n",
        "\n",
        "    # Calculate word and sentence counts\n",
        "    word_count = len(words)\n",
        "    sentence_count = len(sentences)\n",
        "\n",
        "    # Calculate average word and sentence lengths\n",
        "    avg_word_length = sum(len(word) for word in words) / word_count if word_count else 0\n",
        "    avg_sentence_length = word_count / sentence_count if sentence_count else 0\n",
        "\n",
        "    # Calculate counts of long and short words\n",
        "    long_word_threshold = 6\n",
        "    short_word_threshold = 4\n",
        "    long_word_count = sum(1 for word in words if len(word) > long_word_threshold)\n",
        "    short_word_count = sum(1 for word in words if len(word) < short_word_threshold)\n",
        "\n",
        "    # Calculate unique token count and nostop count\n",
        "    unique_tokens = set(words)\n",
        "    nostop_words = [word for word in words if word.lower() not in set(stopwords.words('english'))]\n",
        "    nostop_count = len(nostop_words)\n",
        "    unique_token_count = len(unique_tokens)\n",
        "\n",
        "    # Compile features into a dictionary\n",
        "    features = {\n",
        "        'word_count': word_count,\n",
        "        'unique_token_count': unique_token_count,\n",
        "        'nostop_count': nostop_count,\n",
        "        'avg_sentence_length': avg_sentence_length,\n",
        "        'avg_word_length': avg_word_length,\n",
        "        'sentence_count': sentence_count,\n",
        "        'long_word_count': long_word_count,\n",
        "        'short_word_count': short_word_count\n",
        "    }\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "# Applying Function and Extracting Features\n",
        "train_data['length_features'] = train_data['essay'].apply(length_based_features)\n",
        "train_data[['word_count', 'unique_token_count', 'nostop_count', 'avg_sentence_length', 'avg_word_length', 'sentence_count', 'long_word_count', 'short_word_count']] = train_data['length_features'].apply(pd.Series)\n",
        "train_data[['word_count', 'unique_token_count', 'nostop_count', 'avg_sentence_length', 'avg_word_length', 'sentence_count', 'long_word_count', 'short_word_count']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5a53f51",
      "metadata": {
        "id": "a5a53f51"
      },
      "source": [
        "### 2.1.2) Part of Speech Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d164c8ed",
      "metadata": {
        "id": "d164c8ed",
        "outputId": "a5de5dd8-afb2-426f-e52b-381b984a0583"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>noun</th>\n",
              "      <th>adj</th>\n",
              "      <th>pron</th>\n",
              "      <th>verb</th>\n",
              "      <th>cconj</th>\n",
              "      <th>adv</th>\n",
              "      <th>det</th>\n",
              "      <th>propn</th>\n",
              "      <th>num</th>\n",
              "      <th>intj</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>72</td>\n",
              "      <td>20</td>\n",
              "      <td>41</td>\n",
              "      <td>68</td>\n",
              "      <td>14</td>\n",
              "      <td>21</td>\n",
              "      <td>20</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>96</td>\n",
              "      <td>19</td>\n",
              "      <td>42</td>\n",
              "      <td>85</td>\n",
              "      <td>18</td>\n",
              "      <td>17</td>\n",
              "      <td>35</td>\n",
              "      <td>18</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>72</td>\n",
              "      <td>15</td>\n",
              "      <td>16</td>\n",
              "      <td>53</td>\n",
              "      <td>17</td>\n",
              "      <td>11</td>\n",
              "      <td>27</td>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>126</td>\n",
              "      <td>42</td>\n",
              "      <td>23</td>\n",
              "      <td>100</td>\n",
              "      <td>18</td>\n",
              "      <td>26</td>\n",
              "      <td>43</td>\n",
              "      <td>71</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>107</td>\n",
              "      <td>23</td>\n",
              "      <td>28</td>\n",
              "      <td>87</td>\n",
              "      <td>15</td>\n",
              "      <td>34</td>\n",
              "      <td>54</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   noun  adj  pron  verb  cconj  adv  det  propn  num  intj\n",
              "0    72   20    41    68     14   21   20     12    0     0\n",
              "1    96   19    42    85     18   17   35     18    4     0\n",
              "2    72   15    16    53     17   11   27     14    2     0\n",
              "3   126   42    23   100     18   26   43     71    0     0\n",
              "4   107   23    28    87     15   34   54      9    5     0"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def calculate_pos_features(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    pos_counts = Counter(tag for word, tag in pos_tags)\n",
        "\n",
        "    # Simplify POS tags to match the examples given\n",
        "    pos_features = {\n",
        "        'noun': pos_counts['NN'] + pos_counts['NNS'],\n",
        "        'adj': pos_counts['JJ'],\n",
        "        'pron': pos_counts['PRP'] + pos_counts['PRP$'],\n",
        "        'verb': pos_counts['VB'] + pos_counts['VBD'] + pos_counts['VBG'] + pos_counts['VBN'] + pos_counts['VBP'] + pos_counts['VBZ'],\n",
        "        'cconj': pos_counts['CC'],\n",
        "        'adv': pos_counts['RB'],\n",
        "        'det': pos_counts['DT'],\n",
        "        'propn': pos_counts['NNP'] + pos_counts['NNPS'],\n",
        "        'num': pos_counts['CD'],\n",
        "        'intj': pos_counts['UH'],\n",
        "        # 'ner count' is not possible to calculate with just POS tags. It requires Named Entity Recognition (NER)\n",
        "    }\n",
        "\n",
        "    return pos_features\n",
        "\n",
        "# Applying Function and Extracting Features\n",
        "train_data['pos_features'] = train_data['essay'].apply(calculate_pos_features)\n",
        "train_data[['noun', 'adj', 'pron', 'verb', 'cconj', 'adv', 'det', 'propn', 'num', 'intj']] = train_data['pos_features'].apply(pd.Series)\n",
        "train_data[['noun', 'adj', 'pron', 'verb', 'cconj', 'adv', 'det', 'propn', 'num', 'intj']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e01c6160",
      "metadata": {
        "id": "e01c6160"
      },
      "source": [
        "### 2.1.3) Punctuation Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11a01aa7",
      "metadata": {
        "id": "11a01aa7",
        "outputId": "e90bfa5f-dba1-451b-a75a-43112b0371ca",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>period_count</th>\n",
              "      <th>comma_count</th>\n",
              "      <th>question_mark_count</th>\n",
              "      <th>exclamation_mark_count</th>\n",
              "      <th>colon_count</th>\n",
              "      <th>semicolon_count</th>\n",
              "      <th>parentheses_count</th>\n",
              "      <th>hyphen_count</th>\n",
              "      <th>ellipsis_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10</td>\n",
              "      <td>18</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>18</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>14</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>24</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>30</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   period_count  comma_count  question_mark_count  exclamation_mark_count  \\\n",
              "0            10           18                    2                       4   \n",
              "1            18           12                    1                       1   \n",
              "2            14            9                    0                       0   \n",
              "3            24           13                    1                       2   \n",
              "4            30           13                    0                       0   \n",
              "\n",
              "   colon_count  semicolon_count  parentheses_count  hyphen_count  \\\n",
              "0            1                0                  1             2   \n",
              "1            0                0                  0             1   \n",
              "2            0                0                  0             0   \n",
              "3            0                0                  0             3   \n",
              "4            0                0                  0             2   \n",
              "\n",
              "   ellipsis_count  \n",
              "0               0  \n",
              "1               0  \n",
              "2               0  \n",
              "3               0  \n",
              "4               0  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Function to calculate punctuation-based features\n",
        "def punctuation_features(text):\n",
        "    # Count occurrences of each punctuation mark\n",
        "    period_count = text.count('.')\n",
        "    comma_count = text.count(',')\n",
        "    question_mark_count = text.count('?')\n",
        "    exclamation_mark_count = text.count('!')\n",
        "    quotation_mark_count = text.count('\"') + text.count(\"'\")  # Counting both double and single quotes\n",
        "    colon_count = text.count(':')\n",
        "    semicolon_count = text.count(';')\n",
        "    parentheses_count = text.count('(') + text.count(')')  # Each '(' has a matching ')'\n",
        "    hyphen_count = text.count('-')\n",
        "    ellipsis_count = text.count('...')  # Treating ellipsis as a sequence of three periods\n",
        "\n",
        "    # Compiling features into a dictionary\n",
        "    features = {\n",
        "        'period_count': period_count,\n",
        "        'comma_count': comma_count,\n",
        "        'question_mark_count': question_mark_count,\n",
        "        'exclamation_mark_count': exclamation_mark_count,\n",
        "        'colon_count': colon_count,\n",
        "        'semicolon_count': semicolon_count,\n",
        "        'parentheses_count': parentheses_count // 2,  # Dividing by 2 to count pairs\n",
        "        'hyphen_count': hyphen_count,\n",
        "        'ellipsis_count': ellipsis_count\n",
        "    }\n",
        "\n",
        "    return features\n",
        "\n",
        "# Applying Function and Extracting Features\n",
        "train_data['punctuation_features'] = train_data['essay'].apply(punctuation_features)\n",
        "train_data[['period_count', 'comma_count', 'question_mark_count', 'exclamation_mark_count', 'colon_count', 'semicolon_count', 'parentheses_count', 'hyphen_count', 'ellipsis_count']] = train_data['punctuation_features'].apply(pd.Series)\n",
        "train_data[['period_count', 'comma_count', 'question_mark_count', 'exclamation_mark_count', 'colon_count', 'semicolon_count', 'parentheses_count', 'hyphen_count', 'ellipsis_count']].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dd07b10",
      "metadata": {
        "id": "2dd07b10",
        "outputId": "ce8885a9-b29f-40bf-f705-a37eb4144dc3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_count</th>\n",
              "      <th>unique_token_count</th>\n",
              "      <th>nostop_count</th>\n",
              "      <th>avg_sentence_length</th>\n",
              "      <th>avg_word_length</th>\n",
              "      <th>sentence_count</th>\n",
              "      <th>long_word_count</th>\n",
              "      <th>short_word_count</th>\n",
              "      <th>noun</th>\n",
              "      <th>adj</th>\n",
              "      <th>...</th>\n",
              "      <th>intj</th>\n",
              "      <th>period_count</th>\n",
              "      <th>comma_count</th>\n",
              "      <th>question_mark_count</th>\n",
              "      <th>exclamation_mark_count</th>\n",
              "      <th>colon_count</th>\n",
              "      <th>semicolon_count</th>\n",
              "      <th>parentheses_count</th>\n",
              "      <th>hyphen_count</th>\n",
              "      <th>ellipsis_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>338.0</td>\n",
              "      <td>184.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>30.727273</td>\n",
              "      <td>4.550296</td>\n",
              "      <td>11.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>72</td>\n",
              "      <td>20</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>18</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>419.0</td>\n",
              "      <td>216.0</td>\n",
              "      <td>230.0</td>\n",
              "      <td>22.052632</td>\n",
              "      <td>4.463007</td>\n",
              "      <td>19.0</td>\n",
              "      <td>86.0</td>\n",
              "      <td>169.0</td>\n",
              "      <td>96</td>\n",
              "      <td>19</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>279.0</td>\n",
              "      <td>167.0</td>\n",
              "      <td>139.0</td>\n",
              "      <td>18.600000</td>\n",
              "      <td>4.526882</td>\n",
              "      <td>15.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>72</td>\n",
              "      <td>15</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>524.0</td>\n",
              "      <td>275.0</td>\n",
              "      <td>302.0</td>\n",
              "      <td>20.960000</td>\n",
              "      <td>5.041985</td>\n",
              "      <td>25.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>182.0</td>\n",
              "      <td>126</td>\n",
              "      <td>42</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>465.0</td>\n",
              "      <td>226.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>4.526882</td>\n",
              "      <td>31.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>192.0</td>\n",
              "      <td>107</td>\n",
              "      <td>23</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 27 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   word_count  unique_token_count  nostop_count  avg_sentence_length  \\\n",
              "0       338.0               184.0         170.0            30.727273   \n",
              "1       419.0               216.0         230.0            22.052632   \n",
              "2       279.0               167.0         139.0            18.600000   \n",
              "3       524.0               275.0         302.0            20.960000   \n",
              "4       465.0               226.0         229.0            15.000000   \n",
              "\n",
              "   avg_word_length  sentence_count  long_word_count  short_word_count  noun  \\\n",
              "0         4.550296            11.0             67.0             138.0    72   \n",
              "1         4.463007            19.0             86.0             169.0    96   \n",
              "2         4.526882            15.0             56.0             119.0    72   \n",
              "3         5.041985            25.0            140.0             182.0   126   \n",
              "4         4.526882            31.0             95.0             192.0   107   \n",
              "\n",
              "   adj  ...  intj  period_count  comma_count  question_mark_count  \\\n",
              "0   20  ...     0            10           18                    2   \n",
              "1   19  ...     0            18           12                    1   \n",
              "2   15  ...     0            14            9                    0   \n",
              "3   42  ...     0            24           13                    1   \n",
              "4   23  ...     0            30           13                    0   \n",
              "\n",
              "   exclamation_mark_count  colon_count  semicolon_count  parentheses_count  \\\n",
              "0                       4            1                0                  1   \n",
              "1                       1            0                0                  0   \n",
              "2                       0            0                0                  0   \n",
              "3                       2            0                0                  0   \n",
              "4                       0            0                0                  0   \n",
              "\n",
              "   hyphen_count  ellipsis_count  \n",
              "0             2               0  \n",
              "1             1               0  \n",
              "2             0               0  \n",
              "3             3               0  \n",
              "4             2               0  \n",
              "\n",
              "[5 rows x 27 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_level_features = ['word_count', 'unique_token_count', 'nostop_count', 'avg_sentence_length', 'avg_word_length', 'sentence_count', 'long_word_count', 'short_word_count', 'noun', 'adj', 'pron', 'verb', 'cconj', 'adv', 'det', 'propn', 'num', 'intj', 'period_count', 'comma_count', 'question_mark_count', 'exclamation_mark_count', 'colon_count', 'semicolon_count', 'parentheses_count', 'hyphen_count', 'ellipsis_count']\n",
        "vocab_level_features_df = train_data[vocab_level_features]\n",
        "vocab_level_features_df.to_csv(\"../Data/Features/vocab_level_features_df.csv\", index=False)\n",
        "vocab_level_features_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "810cde45",
      "metadata": {
        "id": "810cde45"
      },
      "source": [
        "## 2.2) Sentence Level Features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fabd1e64",
      "metadata": {
        "id": "fabd1e64"
      },
      "source": [
        "### 2.2.1) Readability Features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdb58055",
      "metadata": {
        "id": "cdb58055"
      },
      "source": [
        "#### 2.2.1.1) Readability Grades"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0fb09bf",
      "metadata": {
        "id": "d0fb09bf",
        "outputId": "0c73f609-7264-492b-c19f-f9cbaa9fbf7c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kincaid</th>\n",
              "      <th>ARI</th>\n",
              "      <th>Coleman_Liau</th>\n",
              "      <th>LIX</th>\n",
              "      <th>Flesch_Reading_Ease</th>\n",
              "      <th>Gunning_Fog</th>\n",
              "      <th>SMOG</th>\n",
              "      <th>RIX</th>\n",
              "      <th>Dale_Chall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8.5</td>\n",
              "      <td>11.3</td>\n",
              "      <td>8.54</td>\n",
              "      <td>42.32</td>\n",
              "      <td>74.02</td>\n",
              "      <td>10.31</td>\n",
              "      <td>10.2</td>\n",
              "      <td>3.93</td>\n",
              "      <td>7.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9.1</td>\n",
              "      <td>10.1</td>\n",
              "      <td>7.95</td>\n",
              "      <td>41.53</td>\n",
              "      <td>67.08</td>\n",
              "      <td>10.21</td>\n",
              "      <td>11.6</td>\n",
              "      <td>4.10</td>\n",
              "      <td>7.28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.7</td>\n",
              "      <td>9.9</td>\n",
              "      <td>8.30</td>\n",
              "      <td>39.97</td>\n",
              "      <td>68.20</td>\n",
              "      <td>10.25</td>\n",
              "      <td>12.0</td>\n",
              "      <td>3.79</td>\n",
              "      <td>7.62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9.7</td>\n",
              "      <td>12.0</td>\n",
              "      <td>10.97</td>\n",
              "      <td>46.12</td>\n",
              "      <td>60.24</td>\n",
              "      <td>10.81</td>\n",
              "      <td>12.3</td>\n",
              "      <td>4.89</td>\n",
              "      <td>8.34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.0</td>\n",
              "      <td>7.7</td>\n",
              "      <td>8.00</td>\n",
              "      <td>35.93</td>\n",
              "      <td>72.66</td>\n",
              "      <td>8.01</td>\n",
              "      <td>11.1</td>\n",
              "      <td>2.93</td>\n",
              "      <td>6.51</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Kincaid   ARI  Coleman_Liau    LIX  Flesch_Reading_Ease  Gunning_Fog  SMOG  \\\n",
              "0      8.5  11.3          8.54  42.32                74.02        10.31  10.2   \n",
              "1      9.1  10.1          7.95  41.53                67.08        10.21  11.6   \n",
              "2      8.7   9.9          8.30  39.97                68.20        10.25  12.0   \n",
              "3      9.7  12.0         10.97  46.12                60.24        10.81  12.3   \n",
              "4      7.0   7.7          8.00  35.93                72.66         8.01  11.1   \n",
              "\n",
              "    RIX  Dale_Chall  \n",
              "0  3.93        7.00  \n",
              "1  4.10        7.28  \n",
              "2  3.79        7.62  \n",
              "3  4.89        8.34  \n",
              "4  2.93        6.51  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def readability_features(text):\n",
        "    features = {\n",
        "        'Kincaid': textstat.flesch_kincaid_grade(text),\n",
        "        'ARI': textstat.automated_readability_index(text),\n",
        "        'Coleman_Liau': textstat.coleman_liau_index(text),\n",
        "        'LIX': textstat.lix(text),\n",
        "        'Flesch_Reading_Ease': textstat.flesch_reading_ease(text),\n",
        "        'Gunning_Fog': textstat.gunning_fog(text),\n",
        "        'SMOG': textstat.smog_index(text),\n",
        "        'RIX': textstat.rix(text),\n",
        "        'Dale_Chall': textstat.dale_chall_readability_score(text)\n",
        "    }\n",
        "    return features\n",
        "\n",
        "# Applying Function and Extracting Features\n",
        "train_data['readability_features'] = train_data['essay'].apply(readability_features)\n",
        "train_data[['Kincaid', 'ARI', 'Coleman_Liau', 'LIX', 'Flesch_Reading_Ease', 'Gunning_Fog', 'SMOG', 'RIX', 'Dale_Chall']] = train_data['readability_features'].apply(pd.Series)\n",
        "train_data[['Kincaid', 'ARI', 'Coleman_Liau', 'LIX', 'Flesch_Reading_Ease', 'Gunning_Fog', 'SMOG', 'RIX', 'Dale_Chall']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d68f22a",
      "metadata": {
        "id": "3d68f22a"
      },
      "source": [
        "#### 2.2.1.2) Sentence Information Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0894839e",
      "metadata": {
        "id": "0894839e",
        "outputId": "ae75cafb-7ab9-4730-88eb-c18ef6b02d35"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>characters_word</th>\n",
              "      <th>syll_word</th>\n",
              "      <th>wordtypes</th>\n",
              "      <th>words_sentence</th>\n",
              "      <th>words</th>\n",
              "      <th>sentences</th>\n",
              "      <th>sentences_paragraph</th>\n",
              "      <th>complex_words</th>\n",
              "      <th>type_token_ratio</th>\n",
              "      <th>characters</th>\n",
              "      <th>syllables</th>\n",
              "      <th>paragraphs</th>\n",
              "      <th>long_words</th>\n",
              "      <th>complex_dc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.984456</td>\n",
              "      <td>1.152850</td>\n",
              "      <td>181.0</td>\n",
              "      <td>24.125000</td>\n",
              "      <td>386.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>0.873057</td>\n",
              "      <td>1538.0</td>\n",
              "      <td>445.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>7.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.030172</td>\n",
              "      <td>1.260776</td>\n",
              "      <td>209.0</td>\n",
              "      <td>23.200000</td>\n",
              "      <td>464.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>0.903017</td>\n",
              "      <td>1870.0</td>\n",
              "      <td>585.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>81.0</td>\n",
              "      <td>7.28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.035144</td>\n",
              "      <td>1.274760</td>\n",
              "      <td>161.0</td>\n",
              "      <td>22.357143</td>\n",
              "      <td>313.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>0.891374</td>\n",
              "      <td>1263.0</td>\n",
              "      <td>399.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>7.62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.328969</td>\n",
              "      <td>1.317512</td>\n",
              "      <td>267.0</td>\n",
              "      <td>22.629630</td>\n",
              "      <td>611.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>0.857610</td>\n",
              "      <td>2645.0</td>\n",
              "      <td>805.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>8.34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.071567</td>\n",
              "      <td>1.276596</td>\n",
              "      <td>211.0</td>\n",
              "      <td>17.233333</td>\n",
              "      <td>517.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>47.0</td>\n",
              "      <td>0.899420</td>\n",
              "      <td>2105.0</td>\n",
              "      <td>660.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>6.51</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   characters_word  syll_word  wordtypes  words_sentence  words  sentences  \\\n",
              "0         3.984456   1.152850      181.0       24.125000  386.0       16.0   \n",
              "1         4.030172   1.260776      209.0       23.200000  464.0       20.0   \n",
              "2         4.035144   1.274760      161.0       22.357143  313.0       14.0   \n",
              "3         4.328969   1.317512      267.0       22.629630  611.0       27.0   \n",
              "4         4.071567   1.276596      211.0       17.233333  517.0       30.0   \n",
              "\n",
              "   sentences_paragraph  complex_words  type_token_ratio  characters  \\\n",
              "0                 16.0           28.0          0.873057      1538.0   \n",
              "1                 20.0           51.0          0.903017      1870.0   \n",
              "2                 14.0           37.0          0.891374      1263.0   \n",
              "3                 27.0           98.0          0.857610      2645.0   \n",
              "4                 30.0           47.0          0.899420      2105.0   \n",
              "\n",
              "   syllables  paragraphs  long_words  complex_dc  \n",
              "0      445.0         1.0        59.0        7.00  \n",
              "1      585.0         1.0        81.0        7.28  \n",
              "2      399.0         1.0        52.0        7.62  \n",
              "3      805.0         1.0       131.0        8.34  \n",
              "4      660.0         1.0        87.0        6.51  "
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def sentence_info_features(text):\n",
        "    words = word_tokenize(text)\n",
        "    syllables = textstat.syllable_count(text)\n",
        "    long_words = sum(1 for word in words if len(word) > 6)\n",
        "    sentences = sent_tokenize(text)\n",
        "    paragraphs = text.split('\\n')\n",
        "\n",
        "    features = {\n",
        "        'characters_word': sum(len(word) for word in words) / len(words) if words else 0,\n",
        "        'syll_word': syllables / len(words) if words else 0,\n",
        "        'wordtypes': len(set(words)),\n",
        "        'words_sentence': len(words) / len(sentences) if sentences else 0,\n",
        "        'words': len(words),\n",
        "        'sentences': len(sentences),\n",
        "        'sentences_paragraph': len(sentences) / len(paragraphs) if paragraphs else 0,\n",
        "        'complex_words': textstat.difficult_words(text),\n",
        "        'type_token_ratio': textstat.lexicon_count(text, removepunct=True) / len(words) if words else 0,\n",
        "        'characters': sum(len(word) for word in words),\n",
        "        'syllables': syllables,\n",
        "        'paragraphs': len(paragraphs),\n",
        "        'long_words': long_words,\n",
        "        'complex_dc': textstat.dale_chall_readability_score(text)\n",
        "    }\n",
        "    return features\n",
        "\n",
        "# Applying Function and Extracting Features\n",
        "train_data['sentence_info_features'] = train_data['essay'].apply(sentence_info_features)\n",
        "train_data[['characters_word', 'syll_word', 'wordtypes', 'words_sentence', 'words', 'sentences', 'sentences_paragraph', 'complex_words', 'type_token_ratio', 'characters', 'syllables', 'paragraphs', 'long_words', 'complex_dc']] = train_data['sentence_info_features'].apply(pd.Series)\n",
        "train_data[['characters_word', 'syll_word', 'wordtypes', 'words_sentence', 'words', 'sentences', 'sentences_paragraph', 'complex_words', 'type_token_ratio', 'characters', 'syllables', 'paragraphs', 'long_words', 'complex_dc']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "261efb7a",
      "metadata": {
        "id": "261efb7a"
      },
      "source": [
        "#### 2.2.1.3) Word Usage Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7a1790b",
      "metadata": {
        "id": "e7a1790b",
        "outputId": "b0d8147f-df70-451b-fbf3-e652b6e5c932"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tobeverb</th>\n",
              "      <th>auxverb</th>\n",
              "      <th>conjunction</th>\n",
              "      <th>pronoun</th>\n",
              "      <th>preposition</th>\n",
              "      <th>nominalization</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>21</td>\n",
              "      <td>5</td>\n",
              "      <td>14</td>\n",
              "      <td>41</td>\n",
              "      <td>53</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>29</td>\n",
              "      <td>16</td>\n",
              "      <td>18</td>\n",
              "      <td>42</td>\n",
              "      <td>58</td>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>13</td>\n",
              "      <td>4</td>\n",
              "      <td>17</td>\n",
              "      <td>16</td>\n",
              "      <td>32</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>31</td>\n",
              "      <td>14</td>\n",
              "      <td>18</td>\n",
              "      <td>23</td>\n",
              "      <td>64</td>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>42</td>\n",
              "      <td>26</td>\n",
              "      <td>15</td>\n",
              "      <td>28</td>\n",
              "      <td>43</td>\n",
              "      <td>58</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   tobeverb  auxverb  conjunction  pronoun  preposition  nominalization\n",
              "0        21        5           14       41           53              36\n",
              "1        29       16           18       42           58              52\n",
              "2        13        4           17       16           32              33\n",
              "3        31       14           18       23           64              90\n",
              "4        42       26           15       28           43              58"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def word_usage_features(text):\n",
        "    words = word_tokenize(text)\n",
        "    tagged_words = pos_tag(words)\n",
        "\n",
        "    pos_counts = Counter(tag for word, tag in tagged_words)\n",
        "\n",
        "    features = {\n",
        "        'tobeverb': pos_counts['VB'],\n",
        "        'auxverb': pos_counts['MD'],\n",
        "        'conjunction': pos_counts['CC'],\n",
        "        'pronoun': pos_counts['PRP'] + pos_counts['PRP$'],\n",
        "        'preposition': pos_counts['IN'],\n",
        "        'nominalization': sum(1 for word, tag in tagged_words if tag.startswith('NN') and len(word) > 6)\n",
        "    }\n",
        "    return features\n",
        "\n",
        "# Applying Function and Extracting Features\n",
        "train_data['word_usage_features'] = train_data['essay'].apply(word_usage_features)\n",
        "train_data[['tobeverb', 'auxverb', 'conjunction', 'pronoun', 'preposition', 'nominalization']] = train_data['word_usage_features'].apply(pd.Series)\n",
        "train_data[['tobeverb', 'auxverb', 'conjunction', 'pronoun', 'preposition', 'nominalization']].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d7d5cc7",
      "metadata": {
        "id": "4d7d5cc7",
        "outputId": "cf26cd55-12d3-4de1-cd22-434669b03b9a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kincaid</th>\n",
              "      <th>ARI</th>\n",
              "      <th>Coleman_Liau</th>\n",
              "      <th>LIX</th>\n",
              "      <th>Flesch_Reading_Ease</th>\n",
              "      <th>Gunning_Fog</th>\n",
              "      <th>SMOG</th>\n",
              "      <th>RIX</th>\n",
              "      <th>Dale_Chall</th>\n",
              "      <th>characters_word</th>\n",
              "      <th>...</th>\n",
              "      <th>syllables</th>\n",
              "      <th>paragraphs</th>\n",
              "      <th>long_words</th>\n",
              "      <th>complex_dc</th>\n",
              "      <th>tobeverb</th>\n",
              "      <th>auxverb</th>\n",
              "      <th>conjunction</th>\n",
              "      <th>pronoun</th>\n",
              "      <th>preposition</th>\n",
              "      <th>nominalization</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8.5</td>\n",
              "      <td>11.3</td>\n",
              "      <td>8.54</td>\n",
              "      <td>42.32</td>\n",
              "      <td>74.02</td>\n",
              "      <td>10.31</td>\n",
              "      <td>10.2</td>\n",
              "      <td>3.93</td>\n",
              "      <td>7.00</td>\n",
              "      <td>3.984456</td>\n",
              "      <td>...</td>\n",
              "      <td>445.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>7.00</td>\n",
              "      <td>21</td>\n",
              "      <td>5</td>\n",
              "      <td>14</td>\n",
              "      <td>41</td>\n",
              "      <td>53</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9.1</td>\n",
              "      <td>10.1</td>\n",
              "      <td>7.95</td>\n",
              "      <td>41.53</td>\n",
              "      <td>67.08</td>\n",
              "      <td>10.21</td>\n",
              "      <td>11.6</td>\n",
              "      <td>4.10</td>\n",
              "      <td>7.28</td>\n",
              "      <td>4.030172</td>\n",
              "      <td>...</td>\n",
              "      <td>585.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>81.0</td>\n",
              "      <td>7.28</td>\n",
              "      <td>29</td>\n",
              "      <td>16</td>\n",
              "      <td>18</td>\n",
              "      <td>42</td>\n",
              "      <td>58</td>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.7</td>\n",
              "      <td>9.9</td>\n",
              "      <td>8.30</td>\n",
              "      <td>39.97</td>\n",
              "      <td>68.20</td>\n",
              "      <td>10.25</td>\n",
              "      <td>12.0</td>\n",
              "      <td>3.79</td>\n",
              "      <td>7.62</td>\n",
              "      <td>4.035144</td>\n",
              "      <td>...</td>\n",
              "      <td>399.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>7.62</td>\n",
              "      <td>13</td>\n",
              "      <td>4</td>\n",
              "      <td>17</td>\n",
              "      <td>16</td>\n",
              "      <td>32</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9.7</td>\n",
              "      <td>12.0</td>\n",
              "      <td>10.97</td>\n",
              "      <td>46.12</td>\n",
              "      <td>60.24</td>\n",
              "      <td>10.81</td>\n",
              "      <td>12.3</td>\n",
              "      <td>4.89</td>\n",
              "      <td>8.34</td>\n",
              "      <td>4.328969</td>\n",
              "      <td>...</td>\n",
              "      <td>805.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>8.34</td>\n",
              "      <td>31</td>\n",
              "      <td>14</td>\n",
              "      <td>18</td>\n",
              "      <td>23</td>\n",
              "      <td>64</td>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.0</td>\n",
              "      <td>7.7</td>\n",
              "      <td>8.00</td>\n",
              "      <td>35.93</td>\n",
              "      <td>72.66</td>\n",
              "      <td>8.01</td>\n",
              "      <td>11.1</td>\n",
              "      <td>2.93</td>\n",
              "      <td>6.51</td>\n",
              "      <td>4.071567</td>\n",
              "      <td>...</td>\n",
              "      <td>660.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>6.51</td>\n",
              "      <td>42</td>\n",
              "      <td>26</td>\n",
              "      <td>15</td>\n",
              "      <td>28</td>\n",
              "      <td>43</td>\n",
              "      <td>58</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 29 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Kincaid   ARI  Coleman_Liau    LIX  Flesch_Reading_Ease  Gunning_Fog  SMOG  \\\n",
              "0      8.5  11.3          8.54  42.32                74.02        10.31  10.2   \n",
              "1      9.1  10.1          7.95  41.53                67.08        10.21  11.6   \n",
              "2      8.7   9.9          8.30  39.97                68.20        10.25  12.0   \n",
              "3      9.7  12.0         10.97  46.12                60.24        10.81  12.3   \n",
              "4      7.0   7.7          8.00  35.93                72.66         8.01  11.1   \n",
              "\n",
              "    RIX  Dale_Chall  characters_word  ...  syllables  paragraphs  long_words  \\\n",
              "0  3.93        7.00         3.984456  ...      445.0         1.0        59.0   \n",
              "1  4.10        7.28         4.030172  ...      585.0         1.0        81.0   \n",
              "2  3.79        7.62         4.035144  ...      399.0         1.0        52.0   \n",
              "3  4.89        8.34         4.328969  ...      805.0         1.0       131.0   \n",
              "4  2.93        6.51         4.071567  ...      660.0         1.0        87.0   \n",
              "\n",
              "   complex_dc  tobeverb  auxverb  conjunction  pronoun  preposition  \\\n",
              "0        7.00        21        5           14       41           53   \n",
              "1        7.28        29       16           18       42           58   \n",
              "2        7.62        13        4           17       16           32   \n",
              "3        8.34        31       14           18       23           64   \n",
              "4        6.51        42       26           15       28           43   \n",
              "\n",
              "   nominalization  \n",
              "0              36  \n",
              "1              52  \n",
              "2              33  \n",
              "3              90  \n",
              "4              58  \n",
              "\n",
              "[5 rows x 29 columns]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Getting all Sent-level features\n",
        "readability_features = ['Kincaid', 'ARI', 'Coleman_Liau', 'LIX', 'Flesch_Reading_Ease', 'Gunning_Fog', 'SMOG', 'RIX', 'Dale_Chall',\n",
        "                               'characters_word', 'syll_word', 'wordtypes', 'words_sentence', 'words', 'sentences', 'sentences_paragraph',\n",
        "                               'complex_words', 'type_token_ratio', 'characters', 'syllables', 'paragraphs', 'long_words', 'complex_dc',\n",
        "                               'tobeverb', 'auxverb', 'conjunction', 'pronoun', 'preposition', 'nominalization']\n",
        "\n",
        "readability_features_df = train_data[readability_features]\n",
        "readability_features_df.to_csv(\"../Data/Features/readability_features_df.csv\", index=False)\n",
        "readability_features_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73581d9c",
      "metadata": {
        "id": "73581d9c"
      },
      "source": [
        "### 2.2.2) Sentence Vector Representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ace92f3",
      "metadata": {
        "id": "7ace92f3"
      },
      "outputs": [],
      "source": [
        "# Preprocessing text\n",
        "def preprocess_text(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    sentences = sent_tokenize(text)\n",
        "    processed_sentences = []\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence)\n",
        "        words = [w.lower() for w in words if w.isalpha() and w.lower() not in stop_words]\n",
        "        if words:\n",
        "            processed_sentences.append(words)\n",
        "    return processed_sentences\n",
        "\n",
        "all_sentences = []\n",
        "for essay in train_data['essay']:\n",
        "    all_sentences.extend(preprocess_text(essay))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8337f5a1",
      "metadata": {
        "id": "8337f5a1"
      },
      "outputs": [],
      "source": [
        "# Training Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences=all_sentences,\n",
        "                          vector_size=100,\n",
        "                          window=5,\n",
        "                          min_count=1,\n",
        "                          workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81438b91",
      "metadata": {
        "id": "81438b91"
      },
      "outputs": [],
      "source": [
        "# Generating Vectors\n",
        "def essay_to_sentence_vectors(essay, model):\n",
        "    processed_sentences = preprocess_text(essay)\n",
        "    essay_sentence_vectors = []\n",
        "    for sentence in processed_sentences:\n",
        "        sentence_vector = np.mean([model.wv[word] for word in sentence if word in model.wv], axis=0)\n",
        "        if not np.isnan(sentence_vector).any():\n",
        "            essay_sentence_vectors.append(sentence_vector)\n",
        "    return essay_sentence_vectors\n",
        "\n",
        "def essay_to_vector(essay, model):\n",
        "    sentence_vectors = essay_to_sentence_vectors(essay, model)\n",
        "    if sentence_vectors:  # Ensure there's at least one sentence vector\n",
        "        essay_vector = np.mean(sentence_vectors, axis=0)\n",
        "    else:\n",
        "        essay_vector = np.zeros(model.vector_size)  # Fallback to a zero vector\n",
        "    return essay_vector\n",
        "\n",
        "# Converting essay to an overall vector\n",
        "sentence_vectors = train_data['essay'].apply(lambda essay: essay_to_vector(essay, word2vec_model))\n",
        "np.save('../Data/Features/sentence_vectors_representation.npy', sentence_vectors.to_numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "315cfbfe",
      "metadata": {
        "id": "315cfbfe"
      },
      "source": [
        "## 2.3) Chapter Level Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30923154",
      "metadata": {
        "id": "30923154"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import LdaModel, LsiModel, Doc2Vec\n",
        "import gensim\n",
        "\n",
        "# Preprocessing text for LDA, LSA, and Doc2Vec\n",
        "stop_words_set = set(stopwords.words('english'))\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Cleans and tokenizes text by removing stopwords and non-alphabetic characters.\n",
        "    \"\"\"\n",
        "    return [token for token in word_tokenize(text.lower()) if token.isalpha() and token not in stop_words_set]\n",
        "\n",
        "processed_essays = train_data['essay'].apply(clean_text)\n",
        "\n",
        "# Creating a dictionary and corpus for topic modeling\n",
        "dictionary = Dictionary(processed_essays)\n",
        "corpus = [dictionary.doc2bow(essay) for essay in processed_essays]\n",
        "\n",
        "# Training LDA and LSA models\n",
        "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, random_state=42, passes=10)\n",
        "lsi_model = LsiModel(corpus=corpus, id2word=dictionary, num_topics=10)\n",
        "\n",
        "# Extracting LDA and LSI features\n",
        "lsa_vectors = []\n",
        "lda_vectors = []\n",
        "for doc in corpus:\n",
        "    lsa = [val[1] for val in lsi_model[doc]]\n",
        "    lda = [val[1] for val in lda_model[doc]]\n",
        "    lsa_vectors.append(lsa)\n",
        "    lda_vectors.append(lda)\n",
        "\n",
        "# Training Doc2Vec model and extracting features\n",
        "documents = [gensim.models.doc2vec.TaggedDocument(doc, [i]) for i, doc in enumerate(processed_essays)]\n",
        "doc2vec_model = Doc2Vec(documents, vector_size=50, window=2, min_count=1, workers=4)\n",
        "doc2vec_vectors = [doc2vec_model.infer_vector(doc.words) for doc in documents]\n",
        "\n",
        "# Converting features into a DataFrame\n",
        "lsa_features_df = pd.DataFrame(lsa_vectors, columns=[f\"LSA_Feature_{i+1}\" for i in range(10)])\n",
        "doc2vec_features_df = pd.DataFrame(doc2vec_vectors, columns=[f\"Doc2Vec_Feature_{i+1}\" for i in range(50)])\n",
        "lda_features_df = pd.DataFrame(lda_vectors, columns=[f\"LDA_Feature_{i+1}\" for i in range(10)])\n",
        "\n",
        "# Combining all extracted chapter-level features into a single DataFrame\n",
        "chapter_level_features = pd.concat([lsa_features_df, lda_features_df, doc2vec_features_df], axis=1)\n",
        "chapter_level_features[\"essary_id\"] = train_data.essay_id\n",
        "chapter_level_features.to_csv(\"../data/features/chapter_features.csv\", index=False)\n",
        "chapter_level_features.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e69879d9",
      "metadata": {
        "id": "e69879d9",
        "outputId": "75de23d2-b67f-4ffe-c37c-49f9f0e0784d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\autom\\AppData\\Local\\Temp\\ipykernel_18052\\1599583875.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  lda_features_df = lda_features_df.append(pd.Series(row_data, index=lda_features_df.columns), ignore_index=True)\n"
          ]
        }
      ],
      "source": [
        "# Saving final data file\n",
        "train_data.to_csv(\"../data/Processed/final_train_dataa.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2055287",
      "metadata": {
        "id": "b2055287"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}