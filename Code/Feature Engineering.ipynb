{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>rater2_trait3</th>\n",
       "      <th>rater2_trait4</th>\n",
       "      <th>rater2_trait5</th>\n",
       "      <th>rater2_trait6</th>\n",
       "      <th>rater3_trait1</th>\n",
       "      <th>rater3_trait2</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "0               4               4             NaN              8   \n",
       "1               5               4             NaN              9   \n",
       "2               4               3             NaN              7   \n",
       "3               5               5             NaN             10   \n",
       "4               4               4             NaN              8   \n",
       "\n",
       "   rater1_domain2  rater2_domain2  domain2_score  ...  rater2_trait3  \\\n",
       "0             NaN             NaN            NaN  ...            NaN   \n",
       "1             NaN             NaN            NaN  ...            NaN   \n",
       "2             NaN             NaN            NaN  ...            NaN   \n",
       "3             NaN             NaN            NaN  ...            NaN   \n",
       "4             NaN             NaN            NaN  ...            NaN   \n",
       "\n",
       "   rater2_trait4  rater2_trait5  rater2_trait6  rater3_trait1  rater3_trait2  \\\n",
       "0            NaN            NaN            NaN            NaN            NaN   \n",
       "1            NaN            NaN            NaN            NaN            NaN   \n",
       "2            NaN            NaN            NaN            NaN            NaN   \n",
       "3            NaN            NaN            NaN            NaN            NaN   \n",
       "4            NaN            NaN            NaN            NaN            NaN   \n",
       "\n",
       "   rater3_trait3  rater3_trait4  rater3_trait5  rater3_trait6  \n",
       "0            NaN            NaN            NaN            NaN  \n",
       "1            NaN            NaN            NaN            NaN  \n",
       "2            NaN            NaN            NaN            NaN  \n",
       "3            NaN            NaN            NaN            NaN  \n",
       "4            NaN            NaN            NaN            NaN  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading Training Data\n",
    "train_data = pd.read_csv(\"../data/raw/training_set_rel3.tsv\", delimiter=\"\\t\", encoding='ISO-8859-1')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting Relevant Columns\n",
    "required_columns = ['essay_id', 'essay_set', 'essay', 'rater1_domain1', 'rater2_domain1', 'domain1_score']\n",
    "train_data = train_data[required_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ATAUMAR\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_words</th>\n",
       "      <th>distinct_word_count</th>\n",
       "      <th>filtered_word_count</th>\n",
       "      <th>avg_sentence_size</th>\n",
       "      <th>avg_word_size</th>\n",
       "      <th>total_sentences</th>\n",
       "      <th>long_words</th>\n",
       "      <th>short_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>338.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>30.727273</td>\n",
       "      <td>4.550296</td>\n",
       "      <td>11.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>138.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>419.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>22.052632</td>\n",
       "      <td>4.463007</td>\n",
       "      <td>19.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>169.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>279.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>4.526882</td>\n",
       "      <td>15.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>119.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>524.0</td>\n",
       "      <td>275.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>20.960000</td>\n",
       "      <td>5.041985</td>\n",
       "      <td>25.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>182.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>465.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>4.526882</td>\n",
       "      <td>31.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>192.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_words  distinct_word_count  filtered_word_count  avg_sentence_size  \\\n",
       "0        338.0                184.0                170.0          30.727273   \n",
       "1        419.0                216.0                230.0          22.052632   \n",
       "2        279.0                167.0                139.0          18.600000   \n",
       "3        524.0                275.0                302.0          20.960000   \n",
       "4        465.0                226.0                229.0          15.000000   \n",
       "\n",
       "   avg_word_size  total_sentences  long_words  short_words  \n",
       "0       4.550296             11.0        67.0        138.0  \n",
       "1       4.463007             19.0        86.0        169.0  \n",
       "2       4.526882             15.0        56.0        119.0  \n",
       "3       5.041985             25.0       140.0        182.0  \n",
       "4       4.526882             31.0        95.0        192.0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Stopwords in English\n",
    "stopword_list = set(stopwords.words('english'))\n",
    "\n",
    "def extract_text_features(text):\n",
    "    \"\"\"\n",
    "    Computes various length-based features for a given essay text.\n",
    "    \"\"\"\n",
    "    # Splitting the text into words and sentences\n",
    "    sentence_list = text.split('.')\n",
    "    word_list = text.split()\n",
    "    \n",
    "    # Counting words and sentences\n",
    "    total_words = len(word_list)\n",
    "    total_sentences = len(sentence_list)\n",
    "    \n",
    "    # Computing average lengths\n",
    "    avg_word_size = sum(len(word) for word in word_list) / total_words if total_words else 0\n",
    "    avg_sentence_size = total_words / total_sentences if total_sentences else 0\n",
    "    \n",
    "    # Categorizing words by length\n",
    "    min_word_size = 4  # Words shorter than this are considered short\n",
    "    max_word_size = 6  # Words longer than this are considered long\n",
    "    long_words = sum(1 for word in word_list if len(word) > max_word_size)\n",
    "    short_words = sum(1 for word in word_list if len(word) < min_word_size)\n",
    "    \n",
    "    # Identifying unique words and non-stopwords\n",
    "    distinct_words = set(word_list)\n",
    "    filtered_words = [word for word in word_list if word.lower() not in stopword_list]\n",
    "    \n",
    "    # Summarizing extracted features\n",
    "    feature_dict = {\n",
    "        'total_words': total_words,\n",
    "        'distinct_word_count': len(distinct_words),\n",
    "        'filtered_word_count': len(filtered_words),\n",
    "        'avg_sentence_size': avg_sentence_size,\n",
    "        'avg_word_size': avg_word_size,\n",
    "        'total_sentences': total_sentences,\n",
    "        'long_words': long_words,\n",
    "        'short_words': short_words\n",
    "    }\n",
    "    \n",
    "    return feature_dict\n",
    "\n",
    "# Applying Feature Extraction to Essays\n",
    "train_data['text_features'] = train_data['essay'].apply(extract_text_features)\n",
    "\n",
    "# Expanding Feature Dictionary into Separate Columns\n",
    "feature_columns = ['total_words', 'distinct_word_count', 'filtered_word_count', \n",
    "                   'avg_sentence_size', 'avg_word_size', 'total_sentences', \n",
    "                   'long_words', 'short_words']\n",
    "\n",
    "train_data[feature_columns] = train_data['text_features'].apply(pd.Series)\n",
    "\n",
    "# Displaying Extracted Features\n",
    "train_data[feature_columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ATAUMAR\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\ATAUMAR\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noun_count</th>\n",
       "      <th>adjective_count</th>\n",
       "      <th>pronoun_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>conjunction_count</th>\n",
       "      <th>adverb_count</th>\n",
       "      <th>determiner_count</th>\n",
       "      <th>proper_noun_count</th>\n",
       "      <th>numeral_count</th>\n",
       "      <th>interjection_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72</td>\n",
       "      <td>20</td>\n",
       "      <td>41</td>\n",
       "      <td>68</td>\n",
       "      <td>14</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96</td>\n",
       "      <td>19</td>\n",
       "      <td>42</td>\n",
       "      <td>85</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>35</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>53</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>27</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>126</td>\n",
       "      <td>42</td>\n",
       "      <td>23</td>\n",
       "      <td>100</td>\n",
       "      <td>18</td>\n",
       "      <td>26</td>\n",
       "      <td>43</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>107</td>\n",
       "      <td>23</td>\n",
       "      <td>28</td>\n",
       "      <td>87</td>\n",
       "      <td>15</td>\n",
       "      <td>34</td>\n",
       "      <td>54</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   noun_count  adjective_count  pronoun_count  verb_count  conjunction_count  \\\n",
       "0          72               20             41          68                 14   \n",
       "1          96               19             42          85                 18   \n",
       "2          72               15             16          53                 17   \n",
       "3         126               42             23         100                 18   \n",
       "4         107               23             28          87                 15   \n",
       "\n",
       "   adverb_count  determiner_count  proper_noun_count  numeral_count  \\\n",
       "0            21                20                 12              0   \n",
       "1            17                35                 18              4   \n",
       "2            11                27                 14              2   \n",
       "3            26                43                 71              0   \n",
       "4            34                54                  9              5   \n",
       "\n",
       "   interjection_count  \n",
       "0                   0  \n",
       "1                   0  \n",
       "2                   0  \n",
       "3                   0  \n",
       "4                   0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk import word_tokenize, pos_tag\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "def extract_pos_features(text):\n",
    "    \"\"\"\n",
    "    Analyzes the Part-of-Speech (POS) distribution in a given text.\n",
    "    \"\"\"\n",
    "    # Tokenizing and tagging words with their respective POS labels\n",
    "    word_list = word_tokenize(text)\n",
    "    tagged_words = pos_tag(word_list)\n",
    "    \n",
    "    # Counting occurrences of each POS category\n",
    "    pos_frequencies = Counter(tag for _, tag in tagged_words)\n",
    "    \n",
    "    # Mapping specific POS categories to broader labels\n",
    "    pos_summary = {\n",
    "        'noun_count': pos_frequencies.get('NN', 0) + pos_frequencies.get('NNS', 0),\n",
    "        'adjective_count': pos_frequencies.get('JJ', 0),\n",
    "        'pronoun_count': pos_frequencies.get('PRP', 0) + pos_frequencies.get('PRP$', 0),\n",
    "        'verb_count': sum(pos_frequencies.get(tag, 0) for tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']),\n",
    "        'conjunction_count': pos_frequencies.get('CC', 0),\n",
    "        'adverb_count': pos_frequencies.get('RB', 0),\n",
    "        'determiner_count': pos_frequencies.get('DT', 0),\n",
    "        'proper_noun_count': pos_frequencies.get('NNP', 0) + pos_frequencies.get('NNPS', 0),\n",
    "        'numeral_count': pos_frequencies.get('CD', 0),\n",
    "        'interjection_count': pos_frequencies.get('UH', 0)\n",
    "    }\n",
    "    \n",
    "    return pos_summary\n",
    "\n",
    "# Applying POS feature extraction to the dataset\n",
    "train_data['pos_analysis'] = train_data['essay'].apply(extract_pos_features)\n",
    "\n",
    "# Expanding the extracted POS data into separate columns\n",
    "pos_columns = ['noun_count', 'adjective_count', 'pronoun_count', 'verb_count', \n",
    "               'conjunction_count', 'adverb_count', 'determiner_count', \n",
    "               'proper_noun_count', 'numeral_count', 'interjection_count']\n",
    "\n",
    "train_data[pos_columns] = train_data['pos_analysis'].apply(pd.Series)\n",
    "train_data[pos_columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
