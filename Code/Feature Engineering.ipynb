{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Training Data\n",
    "train_data = pd.read_csv(\"data/essay_data/training_set_rel3.tsv\", delimiter=\"\\t\", encoding='ISO-8859-1')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting Relevant Columns\n",
    "required_columns = ['essay_id', 'essay_set', 'essay', 'rater1_domain1', 'rater2_domain1', 'domain1_score']\n",
    "train_data = train_data[required_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Stopwords in English\n",
    "stopword_list = set(stopwords.words('english'))\n",
    "\n",
    "def extract_text_features(text):\n",
    "    \"\"\"\n",
    "    Computes various length-based features for a given essay text.\n",
    "    \"\"\"\n",
    "    # Splitting the text into words and sentences\n",
    "    sentence_list = text.split('.')\n",
    "    word_list = text.split()\n",
    "    \n",
    "    # Counting words and sentences\n",
    "    total_words = len(word_list)\n",
    "    total_sentences = len(sentence_list)\n",
    "    \n",
    "    # Computing average lengths\n",
    "    avg_word_size = sum(len(word) for word in word_list) / total_words if total_words else 0\n",
    "    avg_sentence_size = total_words / total_sentences if total_sentences else 0\n",
    "    \n",
    "    # Categorizing words by length\n",
    "    min_word_size = 4  # Words shorter than this are considered short\n",
    "    max_word_size = 6  # Words longer than this are considered long\n",
    "    long_words = sum(1 for word in word_list if len(word) > max_word_size)\n",
    "    short_words = sum(1 for word in word_list if len(word) < min_word_size)\n",
    "    \n",
    "    # Identifying unique words and non-stopwords\n",
    "    distinct_words = set(word_list)\n",
    "    filtered_words = [word for word in word_list if word.lower() not in stopword_list]\n",
    "    \n",
    "    # Summarizing extracted features\n",
    "    feature_dict = {\n",
    "        'total_words': total_words,\n",
    "        'distinct_word_count': len(distinct_words),\n",
    "        'filtered_word_count': len(filtered_words),\n",
    "        'avg_sentence_size': avg_sentence_size,\n",
    "        'avg_word_size': avg_word_size,\n",
    "        'total_sentences': total_sentences,\n",
    "        'long_words': long_words,\n",
    "        'short_words': short_words\n",
    "    }\n",
    "    \n",
    "    return feature_dict\n",
    "\n",
    "# Applying Feature Extraction to Essays\n",
    "train_data['text_features'] = train_data['essay'].apply(extract_text_features)\n",
    "\n",
    "# Expanding Feature Dictionary into Separate Columns\n",
    "feature_columns = ['total_words', 'distinct_word_count', 'filtered_word_count', \n",
    "                   'avg_sentence_size', 'avg_word_size', 'total_sentences', \n",
    "                   'long_words', 'short_words']\n",
    "\n",
    "train_data[feature_columns] = train_data['text_features'].apply(pd.Series)\n",
    "\n",
    "# Displaying Extracted Features\n",
    "train_data[feature_columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
