{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np    # Math operations and arrays\n",
    "import pandas as pd   # To work with tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>rater2_trait3</th>\n",
       "      <th>rater2_trait4</th>\n",
       "      <th>rater2_trait5</th>\n",
       "      <th>rater2_trait6</th>\n",
       "      <th>rater3_trait1</th>\n",
       "      <th>rater3_trait2</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "0               4               4             NaN              8   \n",
       "1               5               4             NaN              9   \n",
       "2               4               3             NaN              7   \n",
       "3               5               5             NaN             10   \n",
       "4               4               4             NaN              8   \n",
       "\n",
       "   rater1_domain2  rater2_domain2  domain2_score  ...  rater2_trait3  \\\n",
       "0             NaN             NaN            NaN  ...            NaN   \n",
       "1             NaN             NaN            NaN  ...            NaN   \n",
       "2             NaN             NaN            NaN  ...            NaN   \n",
       "3             NaN             NaN            NaN  ...            NaN   \n",
       "4             NaN             NaN            NaN  ...            NaN   \n",
       "\n",
       "   rater2_trait4  rater2_trait5  rater2_trait6  rater3_trait1  rater3_trait2  \\\n",
       "0            NaN            NaN            NaN            NaN            NaN   \n",
       "1            NaN            NaN            NaN            NaN            NaN   \n",
       "2            NaN            NaN            NaN            NaN            NaN   \n",
       "3            NaN            NaN            NaN            NaN            NaN   \n",
       "4            NaN            NaN            NaN            NaN            NaN   \n",
       "\n",
       "   rater3_trait3  rater3_trait4  rater3_trait5  rater3_trait6  \n",
       "0            NaN            NaN            NaN            NaN  \n",
       "1            NaN            NaN            NaN            NaN  \n",
       "2            NaN            NaN            NaN            NaN  \n",
       "3            NaN            NaN            NaN            NaN  \n",
       "4            NaN            NaN            NaN            NaN  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading Training Data\n",
    "train_data = pd.read_csv(\"../data/raw/training_set_rel3.tsv\", delimiter=\"\\t\", encoding='ISO-8859-1')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting Relevant Columns\n",
    "required_columns = ['essay_id', 'essay_set', 'essay', 'rater1_domain1', 'rater2_domain1', 'domain1_score']\n",
    "train_data = train_data[required_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Vocabulary Features\n",
    "### a) Length Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.data.path.append(r\"C:\\Users\\ehsan\\OneDrive\\Desktop\\Automated-Grading-of-Exam-Papers-Using-Fusion-based-LSTM-Architecture\\Data\\nltk_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ehsan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_words</th>\n",
       "      <th>distinct_word_count</th>\n",
       "      <th>filtered_word_count</th>\n",
       "      <th>avg_sentence_size</th>\n",
       "      <th>avg_word_size</th>\n",
       "      <th>total_sentences</th>\n",
       "      <th>long_words</th>\n",
       "      <th>short_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>338.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>30.727273</td>\n",
       "      <td>4.550296</td>\n",
       "      <td>11.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>138.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>419.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>22.052632</td>\n",
       "      <td>4.463007</td>\n",
       "      <td>19.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>169.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>279.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>4.526882</td>\n",
       "      <td>15.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>119.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>524.0</td>\n",
       "      <td>275.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>20.960000</td>\n",
       "      <td>5.041985</td>\n",
       "      <td>25.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>182.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>465.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>4.526882</td>\n",
       "      <td>31.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>192.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_words  distinct_word_count  filtered_word_count  avg_sentence_size  \\\n",
       "0        338.0                184.0                170.0          30.727273   \n",
       "1        419.0                216.0                230.0          22.052632   \n",
       "2        279.0                167.0                139.0          18.600000   \n",
       "3        524.0                275.0                302.0          20.960000   \n",
       "4        465.0                226.0                229.0          15.000000   \n",
       "\n",
       "   avg_word_size  total_sentences  long_words  short_words  \n",
       "0       4.550296             11.0        67.0        138.0  \n",
       "1       4.463007             19.0        86.0        169.0  \n",
       "2       4.526882             15.0        56.0        119.0  \n",
       "3       5.041985             25.0       140.0        182.0  \n",
       "4       4.526882             31.0        95.0        192.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Stopwords in English\n",
    "stopword_list = set(stopwords.words('english'))\n",
    "\n",
    "def extract_text_features(text):\n",
    "    \"\"\"\n",
    "    Computes various length-based features for a given essay text.\n",
    "    \"\"\"\n",
    "    # Splitting the text into words and sentences\n",
    "    sentence_list = text.split('.')\n",
    "    word_list = text.split()\n",
    "    \n",
    "    # Counting words and sentences\n",
    "    total_words = len(word_list)\n",
    "    total_sentences = len(sentence_list)\n",
    "    \n",
    "    # Computing average lengths\n",
    "    avg_word_size = sum(len(word) for word in word_list) / total_words if total_words else 0\n",
    "    avg_sentence_size = total_words / total_sentences if total_sentences else 0\n",
    "    \n",
    "    # Categorizing words by length\n",
    "    min_word_size = 4  # Words shorter than this are considered short\n",
    "    max_word_size = 6  # Words longer than this are considered long\n",
    "    long_words = sum(1 for word in word_list if len(word) > max_word_size)\n",
    "    short_words = sum(1 for word in word_list if len(word) < min_word_size)\n",
    "    \n",
    "    # Identifying unique words and non-stopwords\n",
    "    distinct_words = set(word_list)\n",
    "    filtered_words = [word for word in word_list if word.lower() not in stopword_list]\n",
    "    \n",
    "    # Summarizing extracted features\n",
    "    feature_dict = {\n",
    "        'total_words': total_words,\n",
    "        'distinct_word_count': len(distinct_words),\n",
    "        'filtered_word_count': len(filtered_words),\n",
    "        'avg_sentence_size': avg_sentence_size,\n",
    "        'avg_word_size': avg_word_size,\n",
    "        'total_sentences': total_sentences,\n",
    "        'long_words': long_words,\n",
    "        'short_words': short_words\n",
    "    }\n",
    "    \n",
    "    return feature_dict\n",
    "\n",
    "# Applying Feature Extraction to Essays\n",
    "train_data['text_features'] = train_data['essay'].apply(extract_text_features)\n",
    "\n",
    "# Expanding Feature Dictionary into Separate Columns\n",
    "feature_columns = ['total_words', 'distinct_word_count', 'filtered_word_count', \n",
    "                   'avg_sentence_size', 'avg_word_size', 'total_sentences', \n",
    "                   'long_words', 'short_words']\n",
    "\n",
    "train_data[feature_columns] = train_data['text_features'].apply(pd.Series)\n",
    "train_data[feature_columns].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Part of Speech Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ehsan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ehsan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ehsan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noun_count</th>\n",
       "      <th>adjective_count</th>\n",
       "      <th>pronoun_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>conjunction_count</th>\n",
       "      <th>adverb_count</th>\n",
       "      <th>determiner_count</th>\n",
       "      <th>proper_noun_count</th>\n",
       "      <th>numeral_count</th>\n",
       "      <th>interjection_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72</td>\n",
       "      <td>20</td>\n",
       "      <td>41</td>\n",
       "      <td>68</td>\n",
       "      <td>14</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96</td>\n",
       "      <td>19</td>\n",
       "      <td>42</td>\n",
       "      <td>85</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>35</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>53</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>27</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>126</td>\n",
       "      <td>42</td>\n",
       "      <td>23</td>\n",
       "      <td>100</td>\n",
       "      <td>18</td>\n",
       "      <td>26</td>\n",
       "      <td>43</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>107</td>\n",
       "      <td>23</td>\n",
       "      <td>28</td>\n",
       "      <td>87</td>\n",
       "      <td>15</td>\n",
       "      <td>34</td>\n",
       "      <td>54</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   noun_count  adjective_count  pronoun_count  verb_count  conjunction_count  \\\n",
       "0          72               20             41          68                 14   \n",
       "1          96               19             42          85                 18   \n",
       "2          72               15             16          53                 17   \n",
       "3         126               42             23         100                 18   \n",
       "4         107               23             28          87                 15   \n",
       "\n",
       "   adverb_count  determiner_count  proper_noun_count  numeral_count  \\\n",
       "0            21                20                 12              0   \n",
       "1            17                35                 18              4   \n",
       "2            11                27                 14              2   \n",
       "3            26                43                 71              0   \n",
       "4            34                54                  9              5   \n",
       "\n",
       "   interjection_count  \n",
       "0                   0  \n",
       "1                   0  \n",
       "2                   0  \n",
       "3                   0  \n",
       "4                   0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk import word_tokenize, pos_tag\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def extract_pos_features(text):\n",
    "    \"\"\"\n",
    "    Analyzes the Part-of-Speech (POS) distribution in a given text.\n",
    "    \"\"\"\n",
    "    # Tokenizing and tagging words with their respective POS labels\n",
    "    word_list = word_tokenize(text)\n",
    "    tagged_words = pos_tag(word_list)\n",
    "    \n",
    "    # Counting occurrences of each POS category\n",
    "    pos_frequencies = Counter(tag for _, tag in tagged_words)\n",
    "    \n",
    "    # Mapping specific POS categories to broader labels\n",
    "    pos_summary = {\n",
    "        'noun_count': pos_frequencies.get('NN', 0) + pos_frequencies.get('NNS', 0),\n",
    "        'adjective_count': pos_frequencies.get('JJ', 0),\n",
    "        'pronoun_count': pos_frequencies.get('PRP', 0) + pos_frequencies.get('PRP$', 0),\n",
    "        'verb_count': sum(pos_frequencies.get(tag, 0) for tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']),\n",
    "        'conjunction_count': pos_frequencies.get('CC', 0),\n",
    "        'adverb_count': pos_frequencies.get('RB', 0),\n",
    "        'determiner_count': pos_frequencies.get('DT', 0),\n",
    "        'proper_noun_count': pos_frequencies.get('NNP', 0) + pos_frequencies.get('NNPS', 0),\n",
    "        'numeral_count': pos_frequencies.get('CD', 0),\n",
    "        'interjection_count': pos_frequencies.get('UH', 0)\n",
    "    }\n",
    "    \n",
    "    return pos_summary\n",
    "\n",
    "# Applying POS feature extraction to the dataset\n",
    "train_data['pos_analysis'] = train_data['essay'].apply(extract_pos_features)\n",
    "\n",
    "# Expanding the extracted POS data into separate columns\n",
    "pos_columns = ['noun_count', 'adjective_count', 'pronoun_count', 'verb_count', \n",
    "               'conjunction_count', 'adverb_count', 'determiner_count', \n",
    "               'proper_noun_count', 'numeral_count', 'interjection_count']\n",
    "\n",
    "train_data[pos_columns] = train_data['pos_analysis'].apply(pd.Series)\n",
    "train_data[pos_columns].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Punctuation Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dot_count</th>\n",
       "      <th>comma_occurrences</th>\n",
       "      <th>question_marks</th>\n",
       "      <th>exclamations</th>\n",
       "      <th>colon_occurrences</th>\n",
       "      <th>semicolon_occurrences</th>\n",
       "      <th>bracket_pairs</th>\n",
       "      <th>dash_count</th>\n",
       "      <th>triple_dots</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dot_count  comma_occurrences  question_marks  exclamations  \\\n",
       "0         10                 18               2             4   \n",
       "1         18                 12               1             1   \n",
       "2         14                  9               0             0   \n",
       "3         24                 13               1             2   \n",
       "4         30                 13               0             0   \n",
       "\n",
       "   colon_occurrences  semicolon_occurrences  bracket_pairs  dash_count  \\\n",
       "0                  1                      0              1           2   \n",
       "1                  0                      0              0           1   \n",
       "2                  0                      0              0           0   \n",
       "3                  0                      0              0           3   \n",
       "4                  0                      0              0           2   \n",
       "\n",
       "   triple_dots  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to derive punctuation-based characteristics\n",
    "def analyze_punctuation(text):\n",
    "\n",
    "    # Counting different punctuation marks\n",
    "    dot_count = text.count('.')\n",
    "    comma_occurrences = text.count(',')\n",
    "    question_marks = text.count('?')\n",
    "    exclamations = text.count('!')\n",
    "    quote_marks = text.count('\"') + text.count(\"'\")  # Including both single and double quotation marks\n",
    "    colon_occurrences = text.count(':')\n",
    "    semicolon_occurrences = text.count(';')\n",
    "    bracket_pairs = text.count('(') + text.count(')')  # Counting both opening and closing brackets\n",
    "    dash_count = text.count('-')\n",
    "    triple_dots = text.count('...')  # Counting occurrences of ellipsis\n",
    "\n",
    "    # Compiling extracted features into a structured dictionary\n",
    "    punctuation_data = {\n",
    "        'dot_count': dot_count,\n",
    "        'comma_occurrences': comma_occurrences,\n",
    "        'question_marks': question_marks,\n",
    "        'exclamations': exclamations,\n",
    "        'colon_occurrences': colon_occurrences,\n",
    "        'semicolon_occurrences': semicolon_occurrences,\n",
    "        'bracket_pairs': bracket_pairs // 2,  # Adjusting for paired brackets\n",
    "        'dash_count': dash_count,\n",
    "        'triple_dots': triple_dots\n",
    "    }\n",
    "\n",
    "    return punctuation_data\n",
    "\n",
    "# Applying the punctuation feature extraction\n",
    "train_data['punctuation_analysis'] = train_data['essay'].apply(analyze_punctuation)\n",
    "punctuation_columns = ['dot_count', 'comma_occurrences', 'question_marks', \n",
    "                       'exclamations', 'colon_occurrences', 'semicolon_occurrences', \n",
    "                       'bracket_pairs', 'dash_count', 'triple_dots']\n",
    "train_data[punctuation_columns] = train_data['punctuation_analysis'].apply(pd.Series)\n",
    "train_data[punctuation_columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>total_words</th>\n",
       "      <th>distinct_word_count</th>\n",
       "      <th>filtered_word_count</th>\n",
       "      <th>avg_sentence_size</th>\n",
       "      <th>avg_word_size</th>\n",
       "      <th>total_sentences</th>\n",
       "      <th>long_words</th>\n",
       "      <th>short_words</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>...</th>\n",
       "      <th>interjection_count</th>\n",
       "      <th>dot_count</th>\n",
       "      <th>comma_occurrences</th>\n",
       "      <th>question_marks</th>\n",
       "      <th>exclamations</th>\n",
       "      <th>colon_occurrences</th>\n",
       "      <th>semicolon_occurrences</th>\n",
       "      <th>bracket_pairs</th>\n",
       "      <th>dash_count</th>\n",
       "      <th>triple_dots</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>338.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>30.727273</td>\n",
       "      <td>4.550296</td>\n",
       "      <td>11.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>72</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>419.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>22.052632</td>\n",
       "      <td>4.463007</td>\n",
       "      <td>19.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>96</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>279.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>4.526882</td>\n",
       "      <td>15.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>72</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>524.0</td>\n",
       "      <td>275.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>20.960000</td>\n",
       "      <td>5.041985</td>\n",
       "      <td>25.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>126</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>465.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>4.526882</td>\n",
       "      <td>31.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>107</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  total_words  distinct_word_count  filtered_word_count  \\\n",
       "0         1        338.0                184.0                170.0   \n",
       "1         2        419.0                216.0                230.0   \n",
       "2         3        279.0                167.0                139.0   \n",
       "3         4        524.0                275.0                302.0   \n",
       "4         5        465.0                226.0                229.0   \n",
       "\n",
       "   avg_sentence_size  avg_word_size  total_sentences  long_words  short_words  \\\n",
       "0          30.727273       4.550296             11.0        67.0        138.0   \n",
       "1          22.052632       4.463007             19.0        86.0        169.0   \n",
       "2          18.600000       4.526882             15.0        56.0        119.0   \n",
       "3          20.960000       5.041985             25.0       140.0        182.0   \n",
       "4          15.000000       4.526882             31.0        95.0        192.0   \n",
       "\n",
       "   noun_count  ...  interjection_count  dot_count  comma_occurrences  \\\n",
       "0          72  ...                   0         10                 18   \n",
       "1          96  ...                   0         18                 12   \n",
       "2          72  ...                   0         14                  9   \n",
       "3         126  ...                   0         24                 13   \n",
       "4         107  ...                   0         30                 13   \n",
       "\n",
       "   question_marks  exclamations  colon_occurrences  semicolon_occurrences  \\\n",
       "0               2             4                  1                      0   \n",
       "1               1             1                  0                      0   \n",
       "2               0             0                  0                      0   \n",
       "3               1             2                  0                      0   \n",
       "4               0             0                  0                      0   \n",
       "\n",
       "   bracket_pairs  dash_count  triple_dots  \n",
       "0              1           2            0  \n",
       "1              0           1            0  \n",
       "2              0           0            0  \n",
       "3              0           3            0  \n",
       "4              0           2            0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_features = ['essay_id', 'total_words', 'distinct_word_count', 'filtered_word_count', \n",
    "                    'avg_sentence_size', 'avg_word_size', 'total_sentences', \n",
    "                    'long_words', 'short_words', 'noun_count', 'adjective_count', \n",
    "                    'pronoun_count', 'verb_count', 'conjunction_count', \n",
    "                    'adverb_count', 'determiner_count', 'proper_noun_count', \n",
    "                    'numeral_count', 'interjection_count', 'dot_count', \n",
    "                    'comma_occurrences', 'question_marks', 'exclamations', \n",
    "                    'colon_occurrences', 'semicolon_occurrences', 'bracket_pairs', \n",
    "                    'dash_count', 'triple_dots']\n",
    "vocab_features_df = train_data[vocab_features]\n",
    "vocab_features_df.to_csv(\"../data/features/vocabulary_features.csv\", index=False)\n",
    "vocab_features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Sentence Level Features\n",
    "### a) Readability Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textstat -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flesch_kincaid</th>\n",
       "      <th>automated_readability</th>\n",
       "      <th>coleman_liau</th>\n",
       "      <th>lix_score</th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>gunning_fog_index</th>\n",
       "      <th>smog_index</th>\n",
       "      <th>rix_score</th>\n",
       "      <th>dale_chall_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.7</td>\n",
       "      <td>11.3</td>\n",
       "      <td>8.54</td>\n",
       "      <td>42.32</td>\n",
       "      <td>65.56</td>\n",
       "      <td>10.42</td>\n",
       "      <td>10.7</td>\n",
       "      <td>3.93</td>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.1</td>\n",
       "      <td>10.1</td>\n",
       "      <td>7.95</td>\n",
       "      <td>41.53</td>\n",
       "      <td>67.08</td>\n",
       "      <td>10.31</td>\n",
       "      <td>11.8</td>\n",
       "      <td>4.10</td>\n",
       "      <td>7.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.9</td>\n",
       "      <td>9.9</td>\n",
       "      <td>8.30</td>\n",
       "      <td>39.97</td>\n",
       "      <td>59.74</td>\n",
       "      <td>10.54</td>\n",
       "      <td>12.3</td>\n",
       "      <td>3.79</td>\n",
       "      <td>7.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.9</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.97</td>\n",
       "      <td>46.12</td>\n",
       "      <td>51.78</td>\n",
       "      <td>10.97</td>\n",
       "      <td>12.4</td>\n",
       "      <td>4.89</td>\n",
       "      <td>8.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.2</td>\n",
       "      <td>7.7</td>\n",
       "      <td>8.00</td>\n",
       "      <td>35.93</td>\n",
       "      <td>64.20</td>\n",
       "      <td>8.01</td>\n",
       "      <td>11.1</td>\n",
       "      <td>2.93</td>\n",
       "      <td>6.51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   flesch_kincaid  automated_readability  coleman_liau  lix_score  \\\n",
       "0             9.7                   11.3          8.54      42.32   \n",
       "1             9.1                   10.1          7.95      41.53   \n",
       "2             9.9                    9.9          8.30      39.97   \n",
       "3            10.9                   12.0         10.97      46.12   \n",
       "4             8.2                    7.7          8.00      35.93   \n",
       "\n",
       "   flesch_reading_ease  gunning_fog_index  smog_index  rix_score  \\\n",
       "0                65.56              10.42        10.7       3.93   \n",
       "1                67.08              10.31        11.8       4.10   \n",
       "2                59.74              10.54        12.3       3.79   \n",
       "3                51.78              10.97        12.4       4.89   \n",
       "4                64.20               8.01        11.1       2.93   \n",
       "\n",
       "   dale_chall_score  \n",
       "0              7.00  \n",
       "1              7.28  \n",
       "2              7.62  \n",
       "3              8.34  \n",
       "4              6.51  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textstat import flesch_kincaid_grade, automated_readability_index, coleman_liau_index\n",
    "from textstat import lix, flesch_reading_ease, gunning_fog, smog_index, rix, dale_chall_readability_score\n",
    "\n",
    "def compute_readability_metrics(text):\n",
    "    \n",
    "    readability_scores = {\n",
    "        'flesch_kincaid': flesch_kincaid_grade(text),\n",
    "        'automated_readability': automated_readability_index(text),\n",
    "        'coleman_liau': coleman_liau_index(text),\n",
    "        'lix_score': lix(text),\n",
    "        'flesch_reading_ease': flesch_reading_ease(text),\n",
    "        'gunning_fog_index': gunning_fog(text),\n",
    "        'smog_index': smog_index(text),\n",
    "        'rix_score': rix(text),\n",
    "        'dale_chall_score': dale_chall_readability_score(text)\n",
    "    }\n",
    "    return readability_scores\n",
    "\n",
    "# Extracting and storing readability features\n",
    "train_data['readability_metrics'] = train_data['essay'].apply(compute_readability_metrics)\n",
    "readability_columns = ['flesch_kincaid', 'automated_readability', 'coleman_liau', \n",
    "                       'lix_score', 'flesch_reading_ease', 'gunning_fog_index', \n",
    "                       'smog_index', 'rix_score', 'dale_chall_score']\n",
    "\n",
    "train_data[readability_columns] = train_data['readability_metrics'].apply(pd.Series)\n",
    "train_data[readability_columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_char_per_word</th>\n",
       "      <th>avg_syll_per_word</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>avg_words_per_sentence</th>\n",
       "      <th>total_words</th>\n",
       "      <th>total_sentences</th>\n",
       "      <th>avg_sentences_per_paragraph</th>\n",
       "      <th>difficult_word_count</th>\n",
       "      <th>type_token_ratio</th>\n",
       "      <th>total_characters</th>\n",
       "      <th>total_syllables</th>\n",
       "      <th>total_paragraphs</th>\n",
       "      <th>long_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.984456</td>\n",
       "      <td>1.191710</td>\n",
       "      <td>181.0</td>\n",
       "      <td>24.125000</td>\n",
       "      <td>386.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.873057</td>\n",
       "      <td>1538.0</td>\n",
       "      <td>460.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.030172</td>\n",
       "      <td>1.297414</td>\n",
       "      <td>209.0</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>464.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.903017</td>\n",
       "      <td>1870.0</td>\n",
       "      <td>602.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.035144</td>\n",
       "      <td>1.306709</td>\n",
       "      <td>161.0</td>\n",
       "      <td>22.357143</td>\n",
       "      <td>313.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.891374</td>\n",
       "      <td>1263.0</td>\n",
       "      <td>409.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.328969</td>\n",
       "      <td>1.353519</td>\n",
       "      <td>267.0</td>\n",
       "      <td>22.629630</td>\n",
       "      <td>611.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>0.857610</td>\n",
       "      <td>2645.0</td>\n",
       "      <td>827.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>131.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.071567</td>\n",
       "      <td>1.317215</td>\n",
       "      <td>211.0</td>\n",
       "      <td>17.233333</td>\n",
       "      <td>517.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.899420</td>\n",
       "      <td>2105.0</td>\n",
       "      <td>681.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   avg_char_per_word  avg_syll_per_word  unique_word_count  \\\n",
       "0           3.984456           1.191710              181.0   \n",
       "1           4.030172           1.297414              209.0   \n",
       "2           4.035144           1.306709              161.0   \n",
       "3           4.328969           1.353519              267.0   \n",
       "4           4.071567           1.317215              211.0   \n",
       "\n",
       "   avg_words_per_sentence  total_words  total_sentences  \\\n",
       "0               24.125000        386.0             16.0   \n",
       "1               23.200000        464.0             20.0   \n",
       "2               22.357143        313.0             14.0   \n",
       "3               22.629630        611.0             27.0   \n",
       "4               17.233333        517.0             30.0   \n",
       "\n",
       "   avg_sentences_per_paragraph  difficult_word_count  type_token_ratio  \\\n",
       "0                         16.0                  28.0          0.873057   \n",
       "1                         20.0                  52.0          0.903017   \n",
       "2                         14.0                  39.0          0.891374   \n",
       "3                         27.0                 101.0          0.857610   \n",
       "4                         30.0                  50.0          0.899420   \n",
       "\n",
       "   total_characters  total_syllables  total_paragraphs  long_word_count  \n",
       "0            1538.0            460.0               1.0             59.0  \n",
       "1            1870.0            602.0               1.0             81.0  \n",
       "2            1263.0            409.0               1.0             52.0  \n",
       "3            2645.0            827.0               1.0            131.0  \n",
       "4            2105.0            681.0               1.0             87.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from textstat import syllable_count, difficult_words, lexicon_count\n",
    "\n",
    "def extract_sentence_features(text):\n",
    "\n",
    "    # Tokenize words and sentences\n",
    "    tokens = word_tokenize(text)\n",
    "    total_syllables = syllable_count(text)\n",
    "    long_word_count = sum(1 for token in tokens if len(token) > 6)\n",
    "    \n",
    "    sentence_list = sent_tokenize(text)\n",
    "    paragraph_list = text.split('\\n')\n",
    "\n",
    "    # Extracting various metrics related to sentence structure and complexity\n",
    "    sentence_metrics = {\n",
    "        'avg_char_per_word': sum(len(token) for token in tokens) / len(tokens) if tokens else 0,\n",
    "        'avg_syll_per_word': total_syllables / len(tokens) if tokens else 0,\n",
    "        'unique_word_count': len(set(tokens)),\n",
    "        'avg_words_per_sentence': len(tokens) / len(sentence_list) if sentence_list else 0,\n",
    "        'total_words': len(tokens),\n",
    "        'total_sentences': len(sentence_list),\n",
    "        'avg_sentences_per_paragraph': len(sentence_list) / len(paragraph_list) if paragraph_list else 0,\n",
    "        'difficult_word_count': difficult_words(text),\n",
    "        'type_token_ratio': lexicon_count(text, removepunct=True) / len(tokens) if tokens else 0,\n",
    "        'total_characters': sum(len(token) for token in tokens),\n",
    "        'total_syllables': total_syllables,\n",
    "        'total_paragraphs': len(paragraph_list),\n",
    "        'long_word_count': long_word_count\n",
    "    }\n",
    "    \n",
    "    return sentence_metrics\n",
    "\n",
    "# Applying the feature extraction to the dataset\n",
    "train_data['sentence_metrics'] = train_data['essay'].apply(extract_sentence_features)\n",
    "sentence_columns = ['avg_char_per_word', 'avg_syll_per_word', 'unique_word_count', \n",
    "                    'avg_words_per_sentence', 'total_words', 'total_sentences', \n",
    "                    'avg_sentences_per_paragraph', 'difficult_word_count', \n",
    "                    'type_token_ratio', 'total_characters', 'total_syllables', \n",
    "                    'total_paragraphs', 'long_word_count']\n",
    "\n",
    "train_data[sentence_columns] = train_data['sentence_metrics'].apply(pd.Series)\n",
    "train_data[sentence_columns].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Sentence Vector Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'triu' from 'scipy.linalg' (c:\\Users\\ehsan\\anaconda3\\Lib\\site-packages\\scipy\\linalg\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sent_tokenize, word_tokenize\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclean_and_tokenize\u001b[39m(text):\n",
      "File \u001b[1;32mc:\\Users\\ehsan\\anaconda3\\Lib\\site-packages\\gensim\\__init__.py:11\u001b[0m\n\u001b[0;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.3.2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ehsan\\anaconda3\\Lib\\site-packages\\gensim\\corpora\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexedcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmmcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleicorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ehsan\\anaconda3\\Lib\\site-packages\\gensim\\corpora\\indexedcorpus.py:14\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[0;32m     16\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIndexedCorpus\u001b[39;00m(interfaces\u001b[38;5;241m.\u001b[39mCorpusABC):\n",
      "File \u001b[1;32mc:\\Users\\ehsan\\anaconda3\\Lib\\site-packages\\gensim\\interfaces.py:19\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[0;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCorpusABC\u001b[39;00m(utils\u001b[38;5;241m.\u001b[39mSaveLoad):\n",
      "File \u001b[1;32mc:\\Users\\ehsan\\anaconda3\\Lib\\site-packages\\gensim\\matutils.py:20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m entropy\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_blas_funcs, triu\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlapack\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_lapack_funcs\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m psi  \u001b[38;5;66;03m# gamma function utils\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'triu' from 'scipy.linalg' (c:\\Users\\ehsan\\anaconda3\\Lib\\site-packages\\scipy\\linalg\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    \n",
    "    stop_words_set = set(stopwords.words('english'))\n",
    "    sentence_list = sent_tokenize(text)\n",
    "    cleaned_sentences = []\n",
    "    \n",
    "    for sentence in sentence_list:\n",
    "        tokens = word_tokenize(sentence)\n",
    "        filtered_tokens = [token.lower() for token in tokens if token.isalpha() and token.lower() not in stop_words_set] # Stopword removal, Puntuantions removal, lower\n",
    "        if filtered_tokens:\n",
    "            cleaned_sentences.append(filtered_tokens)\n",
    "    \n",
    "    return cleaned_sentences\n",
    "\n",
    "# Collecting all sentences from the essays\n",
    "all_cleaned_sentences = []\n",
    "for essay in train_data['essay']:\n",
    "    all_cleaned_sentences.extend(clean_and_tokenize(essay))\n",
    "\n",
    "# Training a Word2Vec model on the processed sentences\n",
    "word2vec_model = Word2Vec(sentences=all_cleaned_sentences, \n",
    "                          vector_size=100, \n",
    "                          window=5, \n",
    "                          min_count=1, \n",
    "                          workers=4)\n",
    "\n",
    "def generate_sentence_vectors(essay, model):\n",
    "    \"\"\"\n",
    "    Converts sentences in an essay into vectors using the trained Word2Vec model.\n",
    "    \"\"\"\n",
    "    processed_sentences = clean_and_tokenize(essay)\n",
    "    sentence_vectors = []\n",
    "    \n",
    "    for sentence in processed_sentences:\n",
    "        vector = np.mean([model.wv[word] for word in sentence if word in model.wv], axis=0)\n",
    "        if not np.isnan(vector).any():\n",
    "            sentence_vectors.append(vector)\n",
    "    \n",
    "    return sentence_vectors\n",
    "\n",
    "def generate_essay_vector(essay, model):\n",
    "    \"\"\"\n",
    "    Converts an entire essay into a single vector representation.\n",
    "    \"\"\"\n",
    "    sentence_vectors = generate_sentence_vectors(essay, model)\n",
    "    if sentence_vectors:\n",
    "        essay_vector = np.mean(sentence_vectors, axis=0)\n",
    "    else:\n",
    "        essay_vector = np.zeros(model.vector_size)  # Use a zero vector as fallback\n",
    "    \n",
    "    return essay_vector\n",
    "\n",
    "# Creating vector representations for each essay\n",
    "essay_vectors = train_data['essay'].apply(lambda essay: generate_essay_vector(essay, word2vec_model)).apply(pd.Series)\n",
    "\n",
    "# Adding dataframe\n",
    "w2v_columns = [f\"w2v_{i}\" for i in range(1,101)]\n",
    "train_data[w2v_columns] = essay_vectors.apply(pd.Series)\n",
    "train_data[w2v_columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>flesch_kincaid</th>\n",
       "      <th>automated_readability</th>\n",
       "      <th>coleman_liau</th>\n",
       "      <th>lix_score</th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>gunning_fog_index</th>\n",
       "      <th>smog_index</th>\n",
       "      <th>rix_score</th>\n",
       "      <th>dale_chall_score</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v_91</th>\n",
       "      <th>w2v_92</th>\n",
       "      <th>w2v_93</th>\n",
       "      <th>w2v_94</th>\n",
       "      <th>w2v_95</th>\n",
       "      <th>w2v_96</th>\n",
       "      <th>w2v_97</th>\n",
       "      <th>w2v_98</th>\n",
       "      <th>w2v_99</th>\n",
       "      <th>w2v_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>9.7</td>\n",
       "      <td>11.3</td>\n",
       "      <td>8.54</td>\n",
       "      <td>42.32</td>\n",
       "      <td>65.56</td>\n",
       "      <td>10.42</td>\n",
       "      <td>10.7</td>\n",
       "      <td>3.93</td>\n",
       "      <td>7.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.337763</td>\n",
       "      <td>0.165934</td>\n",
       "      <td>0.231120</td>\n",
       "      <td>0.336043</td>\n",
       "      <td>0.838567</td>\n",
       "      <td>0.680200</td>\n",
       "      <td>0.139399</td>\n",
       "      <td>-0.266914</td>\n",
       "      <td>0.615034</td>\n",
       "      <td>-0.079027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>9.1</td>\n",
       "      <td>10.1</td>\n",
       "      <td>7.95</td>\n",
       "      <td>41.53</td>\n",
       "      <td>67.08</td>\n",
       "      <td>10.31</td>\n",
       "      <td>11.8</td>\n",
       "      <td>4.10</td>\n",
       "      <td>7.28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.161736</td>\n",
       "      <td>0.041253</td>\n",
       "      <td>0.215270</td>\n",
       "      <td>0.475975</td>\n",
       "      <td>0.807934</td>\n",
       "      <td>0.297432</td>\n",
       "      <td>0.196454</td>\n",
       "      <td>-0.095835</td>\n",
       "      <td>0.728809</td>\n",
       "      <td>-0.060283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9.9</td>\n",
       "      <td>9.9</td>\n",
       "      <td>8.30</td>\n",
       "      <td>39.97</td>\n",
       "      <td>59.74</td>\n",
       "      <td>10.54</td>\n",
       "      <td>12.3</td>\n",
       "      <td>3.79</td>\n",
       "      <td>7.62</td>\n",
       "      <td>...</td>\n",
       "      <td>0.358576</td>\n",
       "      <td>0.096212</td>\n",
       "      <td>0.112069</td>\n",
       "      <td>0.498785</td>\n",
       "      <td>0.961160</td>\n",
       "      <td>0.425794</td>\n",
       "      <td>0.358720</td>\n",
       "      <td>-0.234130</td>\n",
       "      <td>0.810724</td>\n",
       "      <td>-0.149724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>10.9</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.97</td>\n",
       "      <td>46.12</td>\n",
       "      <td>51.78</td>\n",
       "      <td>10.97</td>\n",
       "      <td>12.4</td>\n",
       "      <td>4.89</td>\n",
       "      <td>8.34</td>\n",
       "      <td>...</td>\n",
       "      <td>0.184646</td>\n",
       "      <td>0.047956</td>\n",
       "      <td>0.045367</td>\n",
       "      <td>0.389533</td>\n",
       "      <td>0.653036</td>\n",
       "      <td>0.301427</td>\n",
       "      <td>0.300964</td>\n",
       "      <td>-0.167743</td>\n",
       "      <td>0.669368</td>\n",
       "      <td>-0.069912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>8.2</td>\n",
       "      <td>7.7</td>\n",
       "      <td>8.00</td>\n",
       "      <td>35.93</td>\n",
       "      <td>64.20</td>\n",
       "      <td>8.01</td>\n",
       "      <td>11.1</td>\n",
       "      <td>2.93</td>\n",
       "      <td>6.51</td>\n",
       "      <td>...</td>\n",
       "      <td>0.221012</td>\n",
       "      <td>0.123638</td>\n",
       "      <td>0.139652</td>\n",
       "      <td>0.578484</td>\n",
       "      <td>0.850886</td>\n",
       "      <td>0.430580</td>\n",
       "      <td>0.120094</td>\n",
       "      <td>-0.255210</td>\n",
       "      <td>0.765925</td>\n",
       "      <td>0.002656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 124 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  flesch_kincaid  automated_readability  coleman_liau  lix_score  \\\n",
       "0         1             9.7                   11.3          8.54      42.32   \n",
       "1         2             9.1                   10.1          7.95      41.53   \n",
       "2         3             9.9                    9.9          8.30      39.97   \n",
       "3         4            10.9                   12.0         10.97      46.12   \n",
       "4         5             8.2                    7.7          8.00      35.93   \n",
       "\n",
       "   flesch_reading_ease  gunning_fog_index  smog_index  rix_score  \\\n",
       "0                65.56              10.42        10.7       3.93   \n",
       "1                67.08              10.31        11.8       4.10   \n",
       "2                59.74              10.54        12.3       3.79   \n",
       "3                51.78              10.97        12.4       4.89   \n",
       "4                64.20               8.01        11.1       2.93   \n",
       "\n",
       "   dale_chall_score  ...    w2v_91    w2v_92    w2v_93    w2v_94    w2v_95  \\\n",
       "0              7.00  ...  0.337763  0.165934  0.231120  0.336043  0.838567   \n",
       "1              7.28  ...  0.161736  0.041253  0.215270  0.475975  0.807934   \n",
       "2              7.62  ...  0.358576  0.096212  0.112069  0.498785  0.961160   \n",
       "3              8.34  ...  0.184646  0.047956  0.045367  0.389533  0.653036   \n",
       "4              6.51  ...  0.221012  0.123638  0.139652  0.578484  0.850886   \n",
       "\n",
       "     w2v_96    w2v_97    w2v_98    w2v_99   w2v_100  \n",
       "0  0.680200  0.139399 -0.266914  0.615034 -0.079027  \n",
       "1  0.297432  0.196454 -0.095835  0.728809 -0.060283  \n",
       "2  0.425794  0.358720 -0.234130  0.810724 -0.149724  \n",
       "3  0.301427  0.300964 -0.167743  0.669368 -0.069912  \n",
       "4  0.430580  0.120094 -0.255210  0.765925  0.002656  \n",
       "\n",
       "[5 rows x 124 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sentence_features = ['essay_id'] + readability_columns + sentence_columns + w2v_columns\n",
    "df_sentence_features = train_data[all_sentence_features]\n",
    "df_sentence_features.to_csv(\"../data/features/sentence_features.csv\", index=False)\n",
    "df_sentence_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
