{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np    # Math operations and arrays\n",
    "import pandas as pd   # To work with tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>rater2_trait3</th>\n",
       "      <th>rater2_trait4</th>\n",
       "      <th>rater2_trait5</th>\n",
       "      <th>rater2_trait6</th>\n",
       "      <th>rater3_trait1</th>\n",
       "      <th>rater3_trait2</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "0               4               4             NaN              8   \n",
       "1               5               4             NaN              9   \n",
       "2               4               3             NaN              7   \n",
       "3               5               5             NaN             10   \n",
       "4               4               4             NaN              8   \n",
       "\n",
       "   rater1_domain2  rater2_domain2  domain2_score  ...  rater2_trait3  \\\n",
       "0             NaN             NaN            NaN  ...            NaN   \n",
       "1             NaN             NaN            NaN  ...            NaN   \n",
       "2             NaN             NaN            NaN  ...            NaN   \n",
       "3             NaN             NaN            NaN  ...            NaN   \n",
       "4             NaN             NaN            NaN  ...            NaN   \n",
       "\n",
       "   rater2_trait4  rater2_trait5  rater2_trait6  rater3_trait1  rater3_trait2  \\\n",
       "0            NaN            NaN            NaN            NaN            NaN   \n",
       "1            NaN            NaN            NaN            NaN            NaN   \n",
       "2            NaN            NaN            NaN            NaN            NaN   \n",
       "3            NaN            NaN            NaN            NaN            NaN   \n",
       "4            NaN            NaN            NaN            NaN            NaN   \n",
       "\n",
       "   rater3_trait3  rater3_trait4  rater3_trait5  rater3_trait6  \n",
       "0            NaN            NaN            NaN            NaN  \n",
       "1            NaN            NaN            NaN            NaN  \n",
       "2            NaN            NaN            NaN            NaN  \n",
       "3            NaN            NaN            NaN            NaN  \n",
       "4            NaN            NaN            NaN            NaN  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading Training Data\n",
    "train_data = pd.read_csv(\"../data/raw/training_set_rel3.tsv\", delimiter=\"\\t\", encoding='ISO-8859-1')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting Relevant Columns\n",
    "required_columns = ['essay_id', 'essay_set', 'essay', 'rater1_domain1', 'rater2_domain1', 'domain1_score']\n",
    "train_data = train_data[required_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ehsan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_words</th>\n",
       "      <th>distinct_word_count</th>\n",
       "      <th>filtered_word_count</th>\n",
       "      <th>avg_sentence_size</th>\n",
       "      <th>avg_word_size</th>\n",
       "      <th>total_sentences</th>\n",
       "      <th>long_words</th>\n",
       "      <th>short_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>338.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>30.727273</td>\n",
       "      <td>4.550296</td>\n",
       "      <td>11.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>138.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>419.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>22.052632</td>\n",
       "      <td>4.463007</td>\n",
       "      <td>19.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>169.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>279.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>4.526882</td>\n",
       "      <td>15.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>119.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>524.0</td>\n",
       "      <td>275.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>20.960000</td>\n",
       "      <td>5.041985</td>\n",
       "      <td>25.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>182.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>465.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>4.526882</td>\n",
       "      <td>31.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>192.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_words  distinct_word_count  filtered_word_count  avg_sentence_size  \\\n",
       "0        338.0                184.0                170.0          30.727273   \n",
       "1        419.0                216.0                230.0          22.052632   \n",
       "2        279.0                167.0                139.0          18.600000   \n",
       "3        524.0                275.0                302.0          20.960000   \n",
       "4        465.0                226.0                229.0          15.000000   \n",
       "\n",
       "   avg_word_size  total_sentences  long_words  short_words  \n",
       "0       4.550296             11.0        67.0        138.0  \n",
       "1       4.463007             19.0        86.0        169.0  \n",
       "2       4.526882             15.0        56.0        119.0  \n",
       "3       5.041985             25.0       140.0        182.0  \n",
       "4       4.526882             31.0        95.0        192.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Stopwords in English\n",
    "stopword_list = set(stopwords.words('english'))\n",
    "\n",
    "def extract_text_features(text):\n",
    "    \"\"\"\n",
    "    Computes various length-based features for a given essay text.\n",
    "    \"\"\"\n",
    "    # Splitting the text into words and sentences\n",
    "    sentence_list = text.split('.')\n",
    "    word_list = text.split()\n",
    "    \n",
    "    # Counting words and sentences\n",
    "    total_words = len(word_list)\n",
    "    total_sentences = len(sentence_list)\n",
    "    \n",
    "    # Computing average lengths\n",
    "    avg_word_size = sum(len(word) for word in word_list) / total_words if total_words else 0\n",
    "    avg_sentence_size = total_words / total_sentences if total_sentences else 0\n",
    "    \n",
    "    # Categorizing words by length\n",
    "    min_word_size = 4  # Words shorter than this are considered short\n",
    "    max_word_size = 6  # Words longer than this are considered long\n",
    "    long_words = sum(1 for word in word_list if len(word) > max_word_size)\n",
    "    short_words = sum(1 for word in word_list if len(word) < min_word_size)\n",
    "    \n",
    "    # Identifying unique words and non-stopwords\n",
    "    distinct_words = set(word_list)\n",
    "    filtered_words = [word for word in word_list if word.lower() not in stopword_list]\n",
    "    \n",
    "    # Summarizing extracted features\n",
    "    feature_dict = {\n",
    "        'total_words': total_words,\n",
    "        'distinct_word_count': len(distinct_words),\n",
    "        'filtered_word_count': len(filtered_words),\n",
    "        'avg_sentence_size': avg_sentence_size,\n",
    "        'avg_word_size': avg_word_size,\n",
    "        'total_sentences': total_sentences,\n",
    "        'long_words': long_words,\n",
    "        'short_words': short_words\n",
    "    }\n",
    "    \n",
    "    return feature_dict\n",
    "\n",
    "# Applying Feature Extraction to Essays\n",
    "train_data['text_features'] = train_data['essay'].apply(extract_text_features)\n",
    "\n",
    "# Expanding Feature Dictionary into Separate Columns\n",
    "feature_columns = ['total_words', 'distinct_word_count', 'filtered_word_count', \n",
    "                   'avg_sentence_size', 'avg_word_size', 'total_sentences', \n",
    "                   'long_words', 'short_words']\n",
    "\n",
    "train_data[feature_columns] = train_data['text_features'].apply(pd.Series)\n",
    "train_data[feature_columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "nltk.data.path.append('../Dataset/nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ehsan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ehsan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Error with downloaded zip file\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ehsan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ehsan/nltk_data'\n    - 'C:\\\\Users\\\\ehsan\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\ehsan\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\ehsan\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ehsan\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - '../Dataset/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pos_summary\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Applying POS feature extraction to the dataset\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos_analysis\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124messay\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(extract_pos_features)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Expanding the extracted POS data into separate columns\u001b[39;00m\n\u001b[0;32m     38\u001b[0m pos_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnoun_count\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madjective_count\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpronoun_count\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverb_count\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     39\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconjunction_count\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madverb_count\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeterminer_count\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     40\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproper_noun_count\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeral_count\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minterjection_count\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4918\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4919\u001b[0m         func,\n\u001b[0;32m   4920\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4921\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4922\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4923\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4924\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1509\u001b[0m )\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[16], line 12\u001b[0m, in \u001b[0;36mextract_pos_features\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03mAnalyzes the Part-of-Speech (POS) distribution in a given text.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Tokenizing and tagging words with their respective POS labels\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m word_list \u001b[38;5;241m=\u001b[39m word_tokenize(text)\n\u001b[0;32m     13\u001b[0m tagged_words \u001b[38;5;241m=\u001b[39m pos_tag(word_list)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Counting occurrences of each POS category\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m _open(resource_url)\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, path \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ehsan/nltk_data'\n    - 'C:\\\\Users\\\\ehsan\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\ehsan\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\ehsan\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ehsan\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - '../Dataset/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk import word_tokenize, pos_tag\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def extract_pos_features(text):\n",
    "    \"\"\"\n",
    "    Analyzes the Part-of-Speech (POS) distribution in a given text.\n",
    "    \"\"\"\n",
    "    # Tokenizing and tagging words with their respective POS labels\n",
    "    word_list = word_tokenize(text)\n",
    "    tagged_words = pos_tag(word_list)\n",
    "    \n",
    "    # Counting occurrences of each POS category\n",
    "    pos_frequencies = Counter(tag for _, tag in tagged_words)\n",
    "    \n",
    "    # Mapping specific POS categories to broader labels\n",
    "    pos_summary = {\n",
    "        'noun_count': pos_frequencies.get('NN', 0) + pos_frequencies.get('NNS', 0),\n",
    "        'adjective_count': pos_frequencies.get('JJ', 0),\n",
    "        'pronoun_count': pos_frequencies.get('PRP', 0) + pos_frequencies.get('PRP$', 0),\n",
    "        'verb_count': sum(pos_frequencies.get(tag, 0) for tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']),\n",
    "        'conjunction_count': pos_frequencies.get('CC', 0),\n",
    "        'adverb_count': pos_frequencies.get('RB', 0),\n",
    "        'determiner_count': pos_frequencies.get('DT', 0),\n",
    "        'proper_noun_count': pos_frequencies.get('NNP', 0) + pos_frequencies.get('NNPS', 0),\n",
    "        'numeral_count': pos_frequencies.get('CD', 0),\n",
    "        'interjection_count': pos_frequencies.get('UH', 0)\n",
    "    }\n",
    "    \n",
    "    return pos_summary\n",
    "\n",
    "# Applying POS feature extraction to the dataset\n",
    "train_data['pos_analysis'] = train_data['essay'].apply(extract_pos_features)\n",
    "\n",
    "# Expanding the extracted POS data into separate columns\n",
    "pos_columns = ['noun_count', 'adjective_count', 'pronoun_count', 'verb_count', \n",
    "               'conjunction_count', 'adverb_count', 'determiner_count', \n",
    "               'proper_noun_count', 'numeral_count', 'interjection_count']\n",
    "\n",
    "train_data[pos_columns] = train_data['pos_analysis'].apply(pd.Series)\n",
    "train_data[pos_columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to derive punctuation-based characteristics\n",
    "def analyze_punctuation(text):\n",
    "\n",
    "    # Counting different punctuation marks\n",
    "    dot_count = text.count('.')\n",
    "    comma_occurrences = text.count(',')\n",
    "    question_marks = text.count('?')\n",
    "    exclamations = text.count('!')\n",
    "    quote_marks = text.count('\"') + text.count(\"'\")  # Including both single and double quotation marks\n",
    "    colon_occurrences = text.count(':')\n",
    "    semicolon_occurrences = text.count(';')\n",
    "    bracket_pairs = text.count('(') + text.count(')')  # Counting both opening and closing brackets\n",
    "    dash_count = text.count('-')\n",
    "    triple_dots = text.count('...')  # Counting occurrences of ellipsis\n",
    "\n",
    "    # Compiling extracted features into a structured dictionary\n",
    "    punctuation_data = {\n",
    "        'dot_count': dot_count,\n",
    "        'comma_occurrences': comma_occurrences,\n",
    "        'question_marks': question_marks,\n",
    "        'exclamations': exclamations,\n",
    "        'colon_occurrences': colon_occurrences,\n",
    "        'semicolon_occurrences': semicolon_occurrences,\n",
    "        'bracket_pairs': bracket_pairs // 2,  # Adjusting for paired brackets\n",
    "        'dash_count': dash_count,\n",
    "        'triple_dots': triple_dots\n",
    "    }\n",
    "\n",
    "    return punctuation_data\n",
    "\n",
    "# Applying the punctuation feature extraction to the dataset\n",
    "train_data['punctuation_analysis'] = train_data['essay'].apply(analyze_punctuation)\n",
    "\n",
    "# Expanding extracted punctuation details into separate columns\n",
    "punctuation_columns = ['dot_count', 'comma_occurrences', 'question_marks', \n",
    "                       'exclamations', 'colon_occurrences', 'semicolon_occurrences', \n",
    "                       'bracket_pairs', 'dash_count', 'triple_dots']\n",
    "\n",
    "train_data[punctuation_columns] = train_data['punctuation_analysis'].apply(pd.Series)\n",
    "train_data[punctuation_columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textual_features = ['total_words', 'distinct_word_count', 'filtered_word_count', \n",
    "                    'avg_sentence_size', 'avg_word_size', 'total_sentences', \n",
    "                    'long_words', 'short_words', 'noun_count', 'adjective_count', \n",
    "                    'pronoun_count', 'verb_count', 'conjunction_count', \n",
    "                    'adverb_count', 'determiner_count', 'proper_noun_count', \n",
    "                    'numeral_count', 'interjection_count', 'dot_count', \n",
    "                    'comma_occurrences', 'question_marks', 'exclamations', \n",
    "                    'colon_occurrences', 'semicolon_occurrences', 'bracket_pairs', \n",
    "                    'dash_count', 'triple_dots']\n",
    "\n",
    "# Creating a DataFrame containing the selected feature set\n",
    "textual_features_df = train_data[textual_features]\n",
    "textual_features_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Level Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textstat -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textstat import flesch_kincaid_grade, automated_readability_index, coleman_liau_index\n",
    "from textstat import lix, flesch_reading_ease, gunning_fog, smog_index, rix, dale_chall_readability_score\n",
    "\n",
    "def compute_readability_metrics(text):\n",
    "    \n",
    "    readability_scores = {\n",
    "        'flesch_kincaid': flesch_kincaid_grade(text),\n",
    "        'automated_readability': automated_readability_index(text),\n",
    "        'coleman_liau': coleman_liau_index(text),\n",
    "        'lix_score': lix(text),\n",
    "        'flesch_reading_ease': flesch_reading_ease(text),\n",
    "        'gunning_fog_index': gunning_fog(text),\n",
    "        'smog_index': smog_index(text),\n",
    "        'rix_score': rix(text),\n",
    "        'dale_chall_score': dale_chall_readability_score(text)\n",
    "    }\n",
    "    return readability_scores\n",
    "\n",
    "# Applying the readability analysis to essays\n",
    "train_data['readability_metrics'] = train_data['essay'].apply(compute_readability_metrics)\n",
    "\n",
    "# Expanding extracted readability scores into separate columns\n",
    "readability_columns = ['flesch_kincaid', 'automated_readability', 'coleman_liau', \n",
    "                       'lix_score', 'flesch_reading_ease', 'gunning_fog_index', \n",
    "                       'smog_index', 'rix_score', 'dale_chall_score']\n",
    "\n",
    "train_data[readability_columns] = train_data['readability_metrics'].apply(pd.Series)\n",
    "\n",
    "# Displaying the first few records of readability analysis\n",
    "train_data[readability_columns].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from textstat import syllable_count, difficult_words, lexicon_count, dale_chall_readability_score\n",
    "\n",
    "def extract_sentence_features(text):\n",
    "\n",
    "    # Tokenize words and sentences\n",
    "    tokens = word_tokenize(text)\n",
    "    total_syllables = syllable_count(text)\n",
    "    long_word_count = sum(1 for token in tokens if len(token) > 6)\n",
    "    sentence_list = sent_tokenize(text)\n",
    "    paragraph_list = text.split('\\n')\n",
    "\n",
    "    # Extracting various metrics related to sentence structure and complexity\n",
    "    sentence_metrics = {\n",
    "        'avg_char_per_word': sum(len(token) for token in tokens) / len(tokens) if tokens else 0,\n",
    "        'avg_syll_per_word': total_syllables / len(tokens) if tokens else 0,\n",
    "        'unique_word_count': len(set(tokens)),\n",
    "        'avg_words_per_sentence': len(tokens) / len(sentence_list) if sentence_list else 0,\n",
    "        'total_words': len(tokens),\n",
    "        'total_sentences': len(sentence_list),\n",
    "        'avg_sentences_per_paragraph': len(sentence_list) / len(paragraph_list) if paragraph_list else 0,\n",
    "        'difficult_word_count': difficult_words(text),\n",
    "        'type_token_ratio': lexicon_count(text, removepunct=True) / len(tokens) if tokens else 0,\n",
    "        'total_characters': sum(len(token) for token in tokens),\n",
    "        'total_syllables': total_syllables,\n",
    "        'total_paragraphs': len(paragraph_list),\n",
    "        'long_word_count': long_word_count,\n",
    "        'dale_chall_score': dale_chall_readability_score(text)\n",
    "    }\n",
    "    \n",
    "    return sentence_metrics\n",
    "\n",
    "# Applying the feature extraction to the dataset\n",
    "train_data['sentence_metrics'] = train_data['essay'].apply(extract_sentence_features)\n",
    "\n",
    "# Expanding the computed metrics into individual columns\n",
    "sentence_columns = ['avg_char_per_word', 'avg_syll_per_word', 'unique_word_count', \n",
    "                    'avg_words_per_sentence', 'total_words', 'total_sentences', \n",
    "                    'avg_sentences_per_paragraph', 'difficult_word_count', \n",
    "                    'type_token_ratio', 'total_characters', 'total_syllables', \n",
    "                    'total_paragraphs', 'long_word_count', 'dale_chall_score']\n",
    "\n",
    "train_data[sentence_columns] = train_data['sentence_metrics'].apply(pd.Series)\n",
    "\n",
    "train_data[sentence_columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "combined_features = ['flesch_kincaid', 'automated_readability', 'coleman_liau', \n",
    "                       'lix_score', 'flesch_reading_ease', 'gunning_fog_index', \n",
    "                       'smog_index', 'rix_score', 'dale_chall_score', 'avg_char_per_word', 'avg_syll_per_word', 'unique_word_count', \n",
    "                    'avg_words_per_sentence', 'total_words', 'total_sentences', \n",
    "                    'avg_sentences_per_paragraph', 'difficult_word_count', \n",
    "                    'type_token_ratio', 'total_characters', 'total_syllables', \n",
    "                    'total_paragraphs', 'long_word_count', 'dale_chall_score']\n",
    "\n",
    "# Creating a DataFrame containing the combined feature set\n",
    "combined_features_df = train_data[combined_features]\n",
    "combined_features_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
