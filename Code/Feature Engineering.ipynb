{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np    # Math operations and arrays\n",
    "import pandas as pd   # To work with tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>rater2_trait3</th>\n",
       "      <th>rater2_trait4</th>\n",
       "      <th>rater2_trait5</th>\n",
       "      <th>rater2_trait6</th>\n",
       "      <th>rater3_trait1</th>\n",
       "      <th>rater3_trait2</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "0               4               4             NaN              8   \n",
       "1               5               4             NaN              9   \n",
       "2               4               3             NaN              7   \n",
       "3               5               5             NaN             10   \n",
       "4               4               4             NaN              8   \n",
       "\n",
       "   rater1_domain2  rater2_domain2  domain2_score  ...  rater2_trait3  \\\n",
       "0             NaN             NaN            NaN  ...            NaN   \n",
       "1             NaN             NaN            NaN  ...            NaN   \n",
       "2             NaN             NaN            NaN  ...            NaN   \n",
       "3             NaN             NaN            NaN  ...            NaN   \n",
       "4             NaN             NaN            NaN  ...            NaN   \n",
       "\n",
       "   rater2_trait4  rater2_trait5  rater2_trait6  rater3_trait1  rater3_trait2  \\\n",
       "0            NaN            NaN            NaN            NaN            NaN   \n",
       "1            NaN            NaN            NaN            NaN            NaN   \n",
       "2            NaN            NaN            NaN            NaN            NaN   \n",
       "3            NaN            NaN            NaN            NaN            NaN   \n",
       "4            NaN            NaN            NaN            NaN            NaN   \n",
       "\n",
       "   rater3_trait3  rater3_trait4  rater3_trait5  rater3_trait6  \n",
       "0            NaN            NaN            NaN            NaN  \n",
       "1            NaN            NaN            NaN            NaN  \n",
       "2            NaN            NaN            NaN            NaN  \n",
       "3            NaN            NaN            NaN            NaN  \n",
       "4            NaN            NaN            NaN            NaN  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading Training Data\n",
    "train_data = pd.read_csv(\"../data/raw/training_set_rel3.tsv\", delimiter=\"\\t\", encoding='ISO-8859-1')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting Relevant Columns\n",
    "required_columns = ['essay_id', 'essay_set', 'essay', 'rater1_domain1', 'rater2_domain1', 'domain1_score']\n",
    "train_data = train_data[required_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ATAUMAR\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_words</th>\n",
       "      <th>distinct_word_count</th>\n",
       "      <th>filtered_word_count</th>\n",
       "      <th>avg_sentence_size</th>\n",
       "      <th>avg_word_size</th>\n",
       "      <th>total_sentences</th>\n",
       "      <th>long_words</th>\n",
       "      <th>short_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>338.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>30.727273</td>\n",
       "      <td>4.550296</td>\n",
       "      <td>11.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>138.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>419.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>22.052632</td>\n",
       "      <td>4.463007</td>\n",
       "      <td>19.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>169.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>279.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>4.526882</td>\n",
       "      <td>15.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>119.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>524.0</td>\n",
       "      <td>275.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>20.960000</td>\n",
       "      <td>5.041985</td>\n",
       "      <td>25.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>182.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>465.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>4.526882</td>\n",
       "      <td>31.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>192.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_words  distinct_word_count  filtered_word_count  avg_sentence_size  \\\n",
       "0        338.0                184.0                170.0          30.727273   \n",
       "1        419.0                216.0                230.0          22.052632   \n",
       "2        279.0                167.0                139.0          18.600000   \n",
       "3        524.0                275.0                302.0          20.960000   \n",
       "4        465.0                226.0                229.0          15.000000   \n",
       "\n",
       "   avg_word_size  total_sentences  long_words  short_words  \n",
       "0       4.550296             11.0        67.0        138.0  \n",
       "1       4.463007             19.0        86.0        169.0  \n",
       "2       4.526882             15.0        56.0        119.0  \n",
       "3       5.041985             25.0       140.0        182.0  \n",
       "4       4.526882             31.0        95.0        192.0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Stopwords in English\n",
    "stopword_list = set(stopwords.words('english'))\n",
    "\n",
    "def extract_text_features(text):\n",
    "    \"\"\"\n",
    "    Computes various length-based features for a given essay text.\n",
    "    \"\"\"\n",
    "    # Splitting the text into words and sentences\n",
    "    sentence_list = text.split('.')\n",
    "    word_list = text.split()\n",
    "    \n",
    "    # Counting words and sentences\n",
    "    total_words = len(word_list)\n",
    "    total_sentences = len(sentence_list)\n",
    "    \n",
    "    # Computing average lengths\n",
    "    avg_word_size = sum(len(word) for word in word_list) / total_words if total_words else 0\n",
    "    avg_sentence_size = total_words / total_sentences if total_sentences else 0\n",
    "    \n",
    "    # Categorizing words by length\n",
    "    min_word_size = 4  # Words shorter than this are considered short\n",
    "    max_word_size = 6  # Words longer than this are considered long\n",
    "    long_words = sum(1 for word in word_list if len(word) > max_word_size)\n",
    "    short_words = sum(1 for word in word_list if len(word) < min_word_size)\n",
    "    \n",
    "    # Identifying unique words and non-stopwords\n",
    "    distinct_words = set(word_list)\n",
    "    filtered_words = [word for word in word_list if word.lower() not in stopword_list]\n",
    "    \n",
    "    # Summarizing extracted features\n",
    "    feature_dict = {\n",
    "        'total_words': total_words,\n",
    "        'distinct_word_count': len(distinct_words),\n",
    "        'filtered_word_count': len(filtered_words),\n",
    "        'avg_sentence_size': avg_sentence_size,\n",
    "        'avg_word_size': avg_word_size,\n",
    "        'total_sentences': total_sentences,\n",
    "        'long_words': long_words,\n",
    "        'short_words': short_words\n",
    "    }\n",
    "    \n",
    "    return feature_dict\n",
    "\n",
    "# Applying Feature Extraction to Essays\n",
    "train_data['text_features'] = train_data['essay'].apply(extract_text_features)\n",
    "\n",
    "# Expanding Feature Dictionary into Separate Columns\n",
    "feature_columns = ['total_words', 'distinct_word_count', 'filtered_word_count', \n",
    "                   'avg_sentence_size', 'avg_word_size', 'total_sentences', \n",
    "                   'long_words', 'short_words']\n",
    "\n",
    "train_data[feature_columns] = train_data['text_features'].apply(pd.Series)\n",
    "train_data[feature_columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ATAUMAR\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ATAUMAR\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ATAUMAR\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk import word_tokenize, pos_tag\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def extract_pos_features(text):\n",
    "    \"\"\"\n",
    "    Analyzes the Part-of-Speech (POS) distribution in a given text.\n",
    "    \"\"\"\n",
    "    # Tokenizing and tagging words with their respective POS labels\n",
    "    word_list = word_tokenize(text)\n",
    "    tagged_words = pos_tag(word_list)\n",
    "    \n",
    "    # Counting occurrences of each POS category\n",
    "    pos_frequencies = Counter(tag for _, tag in tagged_words)\n",
    "    \n",
    "    # Mapping specific POS categories to broader labels\n",
    "    pos_summary = {\n",
    "        'noun_count': pos_frequencies.get('NN', 0) + pos_frequencies.get('NNS', 0),\n",
    "        'adjective_count': pos_frequencies.get('JJ', 0),\n",
    "        'pronoun_count': pos_frequencies.get('PRP', 0) + pos_frequencies.get('PRP$', 0),\n",
    "        'verb_count': sum(pos_frequencies.get(tag, 0) for tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']),\n",
    "        'conjunction_count': pos_frequencies.get('CC', 0),\n",
    "        'adverb_count': pos_frequencies.get('RB', 0),\n",
    "        'determiner_count': pos_frequencies.get('DT', 0),\n",
    "        'proper_noun_count': pos_frequencies.get('NNP', 0) + pos_frequencies.get('NNPS', 0),\n",
    "        'numeral_count': pos_frequencies.get('CD', 0),\n",
    "        'interjection_count': pos_frequencies.get('UH', 0)\n",
    "    }\n",
    "    \n",
    "    return pos_summary\n",
    "\n",
    "# Applying POS feature extraction to the dataset\n",
    "train_data['pos_analysis'] = train_data['essay'].apply(extract_pos_features)\n",
    "\n",
    "# Expanding the extracted POS data into separate columns\n",
    "pos_columns = ['noun_count', 'adjective_count', 'pronoun_count', 'verb_count', \n",
    "               'conjunction_count', 'adverb_count', 'determiner_count', \n",
    "               'proper_noun_count', 'numeral_count', 'interjection_count']\n",
    "\n",
    "train_data[pos_columns] = train_data['pos_analysis'].apply(pd.Series)\n",
    "train_data[pos_columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dot_count</th>\n",
       "      <th>comma_occurrences</th>\n",
       "      <th>question_marks</th>\n",
       "      <th>exclamations</th>\n",
       "      <th>colon_occurrences</th>\n",
       "      <th>semicolon_occurrences</th>\n",
       "      <th>bracket_pairs</th>\n",
       "      <th>dash_count</th>\n",
       "      <th>triple_dots</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dot_count  comma_occurrences  question_marks  exclamations  \\\n",
       "0         10                 18               2             4   \n",
       "1         18                 12               1             1   \n",
       "2         14                  9               0             0   \n",
       "3         24                 13               1             2   \n",
       "4         30                 13               0             0   \n",
       "\n",
       "   colon_occurrences  semicolon_occurrences  bracket_pairs  dash_count  \\\n",
       "0                  1                      0              1           2   \n",
       "1                  0                      0              0           1   \n",
       "2                  0                      0              0           0   \n",
       "3                  0                      0              0           3   \n",
       "4                  0                      0              0           2   \n",
       "\n",
       "   triple_dots  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to derive punctuation-based characteristics\n",
    "def analyze_punctuation(text):\n",
    "\n",
    "    # Counting different punctuation marks\n",
    "    dot_count = text.count('.')\n",
    "    comma_occurrences = text.count(',')\n",
    "    question_marks = text.count('?')\n",
    "    exclamations = text.count('!')\n",
    "    quote_marks = text.count('\"') + text.count(\"'\")  # Including both single and double quotation marks\n",
    "    colon_occurrences = text.count(':')\n",
    "    semicolon_occurrences = text.count(';')\n",
    "    bracket_pairs = text.count('(') + text.count(')')  # Counting both opening and closing brackets\n",
    "    dash_count = text.count('-')\n",
    "    triple_dots = text.count('...')  # Counting occurrences of ellipsis\n",
    "\n",
    "    # Compiling extracted features into a structured dictionary\n",
    "    punctuation_data = {\n",
    "        'dot_count': dot_count,\n",
    "        'comma_occurrences': comma_occurrences,\n",
    "        'question_marks': question_marks,\n",
    "        'exclamations': exclamations,\n",
    "        'colon_occurrences': colon_occurrences,\n",
    "        'semicolon_occurrences': semicolon_occurrences,\n",
    "        'bracket_pairs': bracket_pairs // 2,  # Adjusting for paired brackets\n",
    "        'dash_count': dash_count,\n",
    "        'triple_dots': triple_dots\n",
    "    }\n",
    "\n",
    "    return punctuation_data\n",
    "\n",
    "# Applying the punctuation feature extraction to the dataset\n",
    "train_data['punctuation_analysis'] = train_data['essay'].apply(analyze_punctuation)\n",
    "\n",
    "# Expanding extracted punctuation details into separate columns\n",
    "punctuation_columns = ['dot_count', 'comma_occurrences', 'question_marks', \n",
    "                       'exclamations', 'colon_occurrences', 'semicolon_occurrences', \n",
    "                       'bracket_pairs', 'dash_count', 'triple_dots']\n",
    "\n",
    "train_data[punctuation_columns] = train_data['punctuation_analysis'].apply(pd.Series)\n",
    "train_data[punctuation_columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_words</th>\n",
       "      <th>distinct_word_count</th>\n",
       "      <th>filtered_word_count</th>\n",
       "      <th>avg_sentence_size</th>\n",
       "      <th>avg_word_size</th>\n",
       "      <th>total_sentences</th>\n",
       "      <th>long_words</th>\n",
       "      <th>short_words</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>adjective_count</th>\n",
       "      <th>...</th>\n",
       "      <th>interjection_count</th>\n",
       "      <th>dot_count</th>\n",
       "      <th>comma_occurrences</th>\n",
       "      <th>question_marks</th>\n",
       "      <th>exclamations</th>\n",
       "      <th>colon_occurrences</th>\n",
       "      <th>semicolon_occurrences</th>\n",
       "      <th>bracket_pairs</th>\n",
       "      <th>dash_count</th>\n",
       "      <th>triple_dots</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>338.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>30.727273</td>\n",
       "      <td>4.550296</td>\n",
       "      <td>11.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>72</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>419.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>22.052632</td>\n",
       "      <td>4.463007</td>\n",
       "      <td>19.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>96</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>279.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>4.526882</td>\n",
       "      <td>15.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>72</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>524.0</td>\n",
       "      <td>275.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>20.960000</td>\n",
       "      <td>5.041985</td>\n",
       "      <td>25.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>126</td>\n",
       "      <td>42</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>465.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>4.526882</td>\n",
       "      <td>31.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>107</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_words  distinct_word_count  filtered_word_count  avg_sentence_size  \\\n",
       "0        338.0                184.0                170.0          30.727273   \n",
       "1        419.0                216.0                230.0          22.052632   \n",
       "2        279.0                167.0                139.0          18.600000   \n",
       "3        524.0                275.0                302.0          20.960000   \n",
       "4        465.0                226.0                229.0          15.000000   \n",
       "\n",
       "   avg_word_size  total_sentences  long_words  short_words  noun_count  \\\n",
       "0       4.550296             11.0        67.0        138.0          72   \n",
       "1       4.463007             19.0        86.0        169.0          96   \n",
       "2       4.526882             15.0        56.0        119.0          72   \n",
       "3       5.041985             25.0       140.0        182.0         126   \n",
       "4       4.526882             31.0        95.0        192.0         107   \n",
       "\n",
       "   adjective_count  ...  interjection_count  dot_count  comma_occurrences  \\\n",
       "0               20  ...                   0         10                 18   \n",
       "1               19  ...                   0         18                 12   \n",
       "2               15  ...                   0         14                  9   \n",
       "3               42  ...                   0         24                 13   \n",
       "4               23  ...                   0         30                 13   \n",
       "\n",
       "   question_marks  exclamations  colon_occurrences  semicolon_occurrences  \\\n",
       "0               2             4                  1                      0   \n",
       "1               1             1                  0                      0   \n",
       "2               0             0                  0                      0   \n",
       "3               1             2                  0                      0   \n",
       "4               0             0                  0                      0   \n",
       "\n",
       "   bracket_pairs  dash_count  triple_dots  \n",
       "0              1           2            0  \n",
       "1              0           1            0  \n",
       "2              0           0            0  \n",
       "3              0           3            0  \n",
       "4              0           2            0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textual_features = ['total_words', 'distinct_word_count', 'filtered_word_count', \n",
    "                    'avg_sentence_size', 'avg_word_size', 'total_sentences', \n",
    "                    'long_words', 'short_words', 'noun_count', 'adjective_count', \n",
    "                    'pronoun_count', 'verb_count', 'conjunction_count', \n",
    "                    'adverb_count', 'determiner_count', 'proper_noun_count', \n",
    "                    'numeral_count', 'interjection_count', 'dot_count', \n",
    "                    'comma_occurrences', 'question_marks', 'exclamations', \n",
    "                    'colon_occurrences', 'semicolon_occurrences', 'bracket_pairs', \n",
    "                    'dash_count', 'triple_dots']\n",
    "\n",
    "# Creating a DataFrame containing the selected feature set\n",
    "textual_features_df = train_data[textual_features]\n",
    "textual_features_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Level Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textstat -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flesch_kincaid</th>\n",
       "      <th>automated_readability</th>\n",
       "      <th>coleman_liau</th>\n",
       "      <th>lix_score</th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>gunning_fog_index</th>\n",
       "      <th>smog_index</th>\n",
       "      <th>rix_score</th>\n",
       "      <th>dale_chall_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.7</td>\n",
       "      <td>11.3</td>\n",
       "      <td>8.54</td>\n",
       "      <td>42.32</td>\n",
       "      <td>65.56</td>\n",
       "      <td>10.42</td>\n",
       "      <td>10.7</td>\n",
       "      <td>3.93</td>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.1</td>\n",
       "      <td>10.1</td>\n",
       "      <td>7.95</td>\n",
       "      <td>41.53</td>\n",
       "      <td>67.08</td>\n",
       "      <td>10.31</td>\n",
       "      <td>11.8</td>\n",
       "      <td>4.10</td>\n",
       "      <td>7.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.9</td>\n",
       "      <td>9.9</td>\n",
       "      <td>8.30</td>\n",
       "      <td>39.97</td>\n",
       "      <td>59.74</td>\n",
       "      <td>10.54</td>\n",
       "      <td>12.3</td>\n",
       "      <td>3.79</td>\n",
       "      <td>7.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.9</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.97</td>\n",
       "      <td>46.12</td>\n",
       "      <td>51.78</td>\n",
       "      <td>10.97</td>\n",
       "      <td>12.4</td>\n",
       "      <td>4.89</td>\n",
       "      <td>8.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.2</td>\n",
       "      <td>7.7</td>\n",
       "      <td>8.00</td>\n",
       "      <td>35.93</td>\n",
       "      <td>64.20</td>\n",
       "      <td>8.01</td>\n",
       "      <td>11.1</td>\n",
       "      <td>2.93</td>\n",
       "      <td>6.51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   flesch_kincaid  automated_readability  coleman_liau  lix_score  \\\n",
       "0             9.7                   11.3          8.54      42.32   \n",
       "1             9.1                   10.1          7.95      41.53   \n",
       "2             9.9                    9.9          8.30      39.97   \n",
       "3            10.9                   12.0         10.97      46.12   \n",
       "4             8.2                    7.7          8.00      35.93   \n",
       "\n",
       "   flesch_reading_ease  gunning_fog_index  smog_index  rix_score  \\\n",
       "0                65.56              10.42        10.7       3.93   \n",
       "1                67.08              10.31        11.8       4.10   \n",
       "2                59.74              10.54        12.3       3.79   \n",
       "3                51.78              10.97        12.4       4.89   \n",
       "4                64.20               8.01        11.1       2.93   \n",
       "\n",
       "   dale_chall_score  \n",
       "0              7.00  \n",
       "1              7.28  \n",
       "2              7.62  \n",
       "3              8.34  \n",
       "4              6.51  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textstat import flesch_kincaid_grade, automated_readability_index, coleman_liau_index\n",
    "from textstat import lix, flesch_reading_ease, gunning_fog, smog_index, rix, dale_chall_readability_score\n",
    "\n",
    "def compute_readability_metrics(text):\n",
    "    \n",
    "    readability_scores = {\n",
    "        'flesch_kincaid': flesch_kincaid_grade(text),\n",
    "        'automated_readability': automated_readability_index(text),\n",
    "        'coleman_liau': coleman_liau_index(text),\n",
    "        'lix_score': lix(text),\n",
    "        'flesch_reading_ease': flesch_reading_ease(text),\n",
    "        'gunning_fog_index': gunning_fog(text),\n",
    "        'smog_index': smog_index(text),\n",
    "        'rix_score': rix(text),\n",
    "        'dale_chall_score': dale_chall_readability_score(text)\n",
    "    }\n",
    "    return readability_scores\n",
    "\n",
    "# Applying the readability analysis to essays\n",
    "train_data['readability_metrics'] = train_data['essay'].apply(compute_readability_metrics)\n",
    "\n",
    "# Expanding extracted readability scores into separate columns\n",
    "readability_columns = ['flesch_kincaid', 'automated_readability', 'coleman_liau', \n",
    "                       'lix_score', 'flesch_reading_ease', 'gunning_fog_index', \n",
    "                       'smog_index', 'rix_score', 'dale_chall_score']\n",
    "\n",
    "train_data[readability_columns] = train_data['readability_metrics'].apply(pd.Series)\n",
    "\n",
    "# Displaying the first few records of readability analysis\n",
    "train_data[readability_columns].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from textstat import syllable_count, difficult_words, lexicon_count, dale_chall_readability_score\n",
    "\n",
    "def extract_sentence_features(text):\n",
    "\n",
    "    # Tokenize words and sentences\n",
    "    tokens = word_tokenize(text)\n",
    "    total_syllables = syllable_count(text)\n",
    "    long_word_count = sum(1 for token in tokens if len(token) > 6)\n",
    "    sentence_list = sent_tokenize(text)\n",
    "    paragraph_list = text.split('\\n')\n",
    "\n",
    "    # Extracting various metrics related to sentence structure and complexity\n",
    "    sentence_metrics = {\n",
    "        'avg_char_per_word': sum(len(token) for token in tokens) / len(tokens) if tokens else 0,\n",
    "        'avg_syll_per_word': total_syllables / len(tokens) if tokens else 0,\n",
    "        'unique_word_count': len(set(tokens)),\n",
    "        'avg_words_per_sentence': len(tokens) / len(sentence_list) if sentence_list else 0,\n",
    "        'total_words': len(tokens),\n",
    "        'total_sentences': len(sentence_list),\n",
    "        'avg_sentences_per_paragraph': len(sentence_list) / len(paragraph_list) if paragraph_list else 0,\n",
    "        'difficult_word_count': difficult_words(text),\n",
    "        'type_token_ratio': lexicon_count(text, removepunct=True) / len(tokens) if tokens else 0,\n",
    "        'total_characters': sum(len(token) for token in tokens),\n",
    "        'total_syllables': total_syllables,\n",
    "        'total_paragraphs': len(paragraph_list),\n",
    "        'long_word_count': long_word_count,\n",
    "        'dale_chall_score': dale_chall_readability_score(text)\n",
    "    }\n",
    "    \n",
    "    return sentence_metrics\n",
    "\n",
    "# Applying the feature extraction to the dataset\n",
    "train_data['sentence_metrics'] = train_data['essay'].apply(extract_sentence_features)\n",
    "\n",
    "# Expanding the computed metrics into individual columns\n",
    "sentence_columns = ['avg_char_per_word', 'avg_syll_per_word', 'unique_word_count', \n",
    "                    'avg_words_per_sentence', 'total_words', 'total_sentences', \n",
    "                    'avg_sentences_per_paragraph', 'difficult_word_count', \n",
    "                    'type_token_ratio', 'total_characters', 'total_syllables', \n",
    "                    'total_paragraphs', 'long_word_count', 'dale_chall_score']\n",
    "\n",
    "train_data[sentence_columns] = train_data['sentence_metrics'].apply(pd.Series)\n",
    "\n",
    "train_data[sentence_columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "combined_features = ['flesch_kincaid', 'automated_readability', 'coleman_liau', \n",
    "                       'lix_score', 'flesch_reading_ease', 'gunning_fog_index', \n",
    "                       'smog_index', 'rix_score', 'dale_chall_score', 'avg_char_per_word', 'avg_syll_per_word', 'unique_word_count', \n",
    "                    'avg_words_per_sentence', 'total_words', 'total_sentences', \n",
    "                    'avg_sentences_per_paragraph', 'difficult_word_count', \n",
    "                    'type_token_ratio', 'total_characters', 'total_syllables', \n",
    "                    'total_paragraphs', 'long_word_count', 'dale_chall_score']\n",
    "\n",
    "# Creating a DataFrame containing the combined feature set\n",
    "combined_features_df = train_data[combined_features]\n",
    "combined_features_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
